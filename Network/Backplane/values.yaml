envoy-gw:
  enabled: true
  deployment:
    envoyGateway:
      cert:
        expiryDays: 365
      image:
        repository: docker.io/envoyproxy/gateway-dev
        tag: 5b6a35f
      imagePullPolicy: Always
      resources:
        requests:
          cpu: 15m
          memory: 300M
        limits:
          cpu: 200m
          memory: 300M
    ports:
      - name: grpc
        port: 18000
        targetPort: 18000
      - name: ratelimit
        port: 18001
        targetPort: 18001
    replicas: 1
    priorityClassName: 'system-cluster-critical'
    pod:
      annotations: {}
      labels: {}

  config:
    envoyGateway:
      gateway:
        controllerName: gateway.envoyproxy.io/gatewayclass-controller
      provider:
        type: Kubernetes
      extensionApis:
        enableEnvoyPatchPolicy: true
        enableBackend: true
      logging:
        level:
          default: error

  envoyGatewayMetricsService:
    ports:
      - name: http
        port: 19001
        protocol: TCP
        targetPort: 19001

  createNamespace: false

  kubernetesClusterDomain: cluster.local

purelb:
  enabled: true

  # Default values for purelb.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  # Docker image configuration
  image:
    repository: registry.gitlab.com/purelb/purelb
    pullPolicy: Always

  nameOverride: ""
  fullnameOverride: ""

  Prometheus:
    allocator:
      # Metrics service
      Metrics:
        enabled: false

      ## ServiceMonitor
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md
      ## Note: requires Prometheus Operator to be able to work, for example:
      ## helm install prometheus prometheus-community/kube-prometheus-stack \
      ##   --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \
      ##   --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false
      serviceMonitor:
        ## Toggle the ServiceMonitor true if you have Prometheus Operator installed and configured
        enabled: false

        ## Specify the labels to add to the ServiceMonitors to be selected for target discovery
        extraLabels: {}

        ## Specify the endpoints
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/design.md#servicemonitor
        endpoints: []
        ## Sample
        ## endpoints:
        ## - port: metrics
        ##   path: /metrics
        ##   scheme: http
      prometheusRules:
        ## Toggle the prometheusRules to true if you have Prometheus Operator installed and configured
        ##
        enabled: false
        ## Specify the namespace where to add to the prometheusRules
        ##
        namespace: ""
        ## Specify the labels to add to the prometheusRules to be selected for target discovery
        extraLabels: {}
        ## Define here the Custom Prometheus rules
        ## e.g:
        ## rules:
        ##   - alert: PurelbServiceGroupHigh
        ##     expr: purelb_address_pool_addresses_in_use * 100 / purelb_address_pool_size > 90
        ##     for: 2m
        ##     labels:
        ##       severity: critical
        ##     annotations:
        ##       summary: PureLB allocator {{`{{`}} $labels.instance {{`}}`}} as high usage of pool
        ##       description: PureLB allocator {{`{{`}} $labels.instance {{`}}`}} as high usage of pool
        rules: []

    lbnodeagent:
      # Metrics service
      Metrics:
        enabled: false

      ## ServiceMonitor
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md
      ## Note: requires Prometheus Operator to be able to work, for example:
      ## helm install prometheus prometheus-community/kube-prometheus-stack \
      ##   --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \
      ##   --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false
      serviceMonitor:
        ## Toggle the ServiceMonitor true if you have Prometheus Operator installed and configured
        enabled: false

        ## Specify the labels to add to the ServiceMonitors to be selected for target discovery
        extraLabels: {}

        ## Specify the endpoints
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/design.md#servicemonitor
        endpoints: []
        ## Sample
        ## endpoints:
        ## - port: metrics
        ##   path: /metrics
        ##   scheme: http
      prometheusRules:
        ## Toggle the prometheusRules true if you have Prometheus Operator installed and configured
        ##
        enabled: false
        ## Specify the namespace where to add to the prometheusRules
        ##
        namespace: ""
        ## Specify the labels to add to the prometheusRules to be selected for target discovery
        extraLabels: {}
        ## Define here the Custom Prometheus rules
        ## e.g:
        ## rules:
        ##   - alert: PurelbServiceGroupHigh
        ##     expr: purelb_address_pool_addresses_in_use * 100 / purelb_address_pool_size > 90
        ##     for: 2m
        ##     labels:
        ##       severity: critical
        ##     annotations:
        ##       summary: PureLB instance {{ "{{ $labels.instance }}" }} down
        ##       description: Redis&trade; instance {{ "{{ $labels.instance }}" }} is down
        rules: []

  # You may define a valid spec and set create: true to create a ServiceGroup.
  # See https://purelb.gitlab.io/purelb/install/config/
  serviceGroup:
    name: "default"
    create: false
    # For example, to configure a "default" ServiceGroup, comment the above line and uncomment the following lines:
    # create: true
    # spec:
    #   local:
    #     subnet: '192.168.254.0/24'
    #     pool: '192.168.254.200-192.168.254.201'
    #     aggregation: default

  # This can be used to define a list of arbitrary extra Kubernetes objects to be created (configmaps, serviceGroups, etc.).
  # Example:
  #   extraObjects:
  #     - |
  #       apiVersion: v1
  #       kind: ConfigMap
  #       metadata:
  #         name: {{ .Release.Name }}-extra
  #       data:
  #         hello: world
  #     - |
  #       apiVersion: purelb.io/v1
  #       kind: ServiceGroup
  #       metadata:
  #         name: private
  #       spec:
  #         local:
  #           pool: 192.168.0.0-192.168.0.5
  #           subnet: 192.168.0.0/27
  extraObjects: []

  # PureLB will act as the default service announcer if this value is
  # "PureLB". This means that PureLB will handle services that do not
  # have a Spec.LoadBalancerClass field. If this is other than "PureLB"
  # then PureLB will handle only those services that have a
  # Spec.LoadBalancerClass explictly set to "purelb.io/purelb".
  defaultAnnouncer: "PureLB"

  # This value is passed into the memberlist package. Quoting from the
  # memberlist docs:
  #
  #   SecretKey is used to initialize the primary encryption key in a
  #   keyring.  The primary encryption key is the only key used to
  #   encrypt messages and the first key used while attempting to
  #   decrypt messages. Providing a value for this primary key will
  #   enable message-level encryption and verification, and
  #   automatically install the key onto the keyring.  The value should
  #   be either 16, 24, or 32 bytes to select AES-128, AES-192, or
  #   AES-256.
  #
  # See PureLB's internal/election package docs for more info on how
  # PureLB uses memberlist.
  #
  # This random default value is fine for most purposes but for extra
  # security you can replace it with your own value.
  memberlistSecretKey: "8sb7ikA5qHwQQqxc"

  # Optional priorityClass to use for both allocator and lbnodeagent pods.
  # https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/
  priorityClassName: system-cluster-critical

  # Configurable values specific to lbnodeagent.
  # See https://purelb.gitlab.io/purelb/install/config/
  lbnodeagent:
    localint: default
    extlbint: kube-lb0
    sendgarp: false
    podSecurityPolicy:
      enabled: false
    resources:
      limits:
        memory: 100Mi
    containerSecurityContext:
      capabilities:
        add:
        - NET_ADMIN
        - NET_RAW
        drop:
        - ALL
      readOnlyRootFilesystem: false
    # Tolerations: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []
    # For example, to allow lbnodeagents to run on master nodes, comment the above line and uncomment the following lines:
    # tolerations:
    # - effect: NoSchedule
    #   key: node-role.kubernetes.io/master
    # nodeSelector: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    nodeSelector:
    # For example, run lbnodeagent only on nodes with a specific label:
    # nodeSelector:
    #   some.example/label: "value"


  # Configurable values specific to allocator.
  allocator:
    podSecurityPolicy:
      enabled: false
    resources:
      limits:
        memory: 100Mi
    # Set the container security context
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - all
      readOnlyRootFilesystem: true
    # Tolerations: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []
    # For example, to allow the allocator to run on master nodes, comment the above line and uncomment the following lines:
    # tolerations:
    # - effect: NoSchedule
    #   key: node-role.kubernetes.io/master

    # Set the pod security context
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534

commonLabels:
  core-env: 'prod'

cilium:
  # File generated by install/kubernetes/Makefile; DO NOT EDIT.
  # This file is based on install/kubernetes/cilium/*values.yaml.tmpl.


  # @schema
  # type: [null, string]
  # @schema
  # -- namespaceOverride allows to override the destination namespace for Cilium resources.
  # This property allows to use Cilium as part of an Umbrella Chart with different targets.
  namespaceOverride: ""

  commonLabels:
    core-env: 'prod'

  # @schema
  # type: [null, string]
  # @schema
  # -- upgradeCompatibility helps users upgrading to ensure that the configMap for
  # Cilium will not change critical values to ensure continued operation
  # This flag is not required for new installations.
  # For example: '1.7', '1.8', '1.9'
  upgradeCompatibility: null
  debug:
    # -- Enable debug logging
    enabled: false
    # @schema
    # type: [null, string]
    # @schema
    # -- Configure verbosity levels for debug logging
    # This option is used to enable debug messages for operations related to such
    # sub-system such as (e.g. kvstore, envoy, datapath or policy), and flow is
    # for enabling debug messages emitted per request, message and connection.
    # Multiple values can be set via a space-separated string (e.g. "datapath envoy").
    #
    # Applicable values:
    # - flow
    # - kvstore
    # - envoy
    # - datapath
    # - policy
    verbose: ~
  rbac:
    # -- Enable creation of Resource-Based Access Control configuration.
    create: true
  # -- Configure image pull secrets for pulling container images
  imagePullSecrets: []
  # - name: "image-pull-secret"

  # -- Configure iptables--random-fully. Disabled by default. View https://github.com/cilium/cilium/issues/13037 for more information.
  iptablesRandomFully: true

  # -- (string) Kubernetes config path
  # @default -- `"~/.kube/config"`
  #kubeConfigPath: ""
  # -- (string) Kubernetes service host - use "auto" for automatic lookup from the cluster-info ConfigMap
  k8sServiceHost: "172.31.241.238"
  # @schema
  # type: [string, integer]
  # @schema
  # -- (string) Kubernetes service port
  k8sServicePort: "6443"
  # @schema
  # type: [null, string]
  # @schema
  # -- (string) When `k8sServiceHost=auto`, allows to customize the configMap name. It defaults to `cluster-info`.
  k8sServiceLookupConfigMapName: ""
  # @schema
  # type: [null, string]
  # @schema
  # -- (string) When `k8sServiceHost=auto`, allows to customize the namespace that contains `k8sServiceLookupConfigMapName`. It defaults to `kube-public`.
  k8sServiceLookupNamespace: ""
  # -- Configure the client side rate limit for the agent
  #
  # If the amount of requests to the Kubernetes API server exceeds the configured
  # rate limit, the agent will start to throttle requests by delaying
  # them until there is budget or the request times out.
  k8sClientRateLimit:
    # @schema
    # type: [null, integer]
    # @schema
    # -- (int) The sustained request rate in requests per second.
    # @default -- 10
    qps:
    # @schema
    # type: [null, integer]
    # @schema
    # -- (int) The burst request rate in requests per second.
    # The rate limiter will allow short bursts with a higher rate.
    # @default -- 20
    burst:
    # -- Configure the client side rate limit for the Cilium Operator
    operator:
      # @schema
      # type: [null, integer]
      # @schema
      # -- (int) The sustained request rate in requests per second.
      # @default -- 100
      qps:
      # @schema
      # type: [null, integer]
      # @schema
      # -- (int) The burst request rate in requests per second.
      # The rate limiter will allow short bursts with a higher rate.
      # @default -- 200
      burst:
  cluster:
    # -- Name of the cluster. Only required for Cluster Mesh and mutual authentication with SPIRE.
    # It must respect the following constraints:
    # * It must contain at most 32 characters;
    # * It must begin and end with a lower case alphanumeric character;
    # * It may contain lower case alphanumeric characters and dashes between.
    # The "default" name cannot be used if the Cluster ID is different from 0.
    name: default
    # -- (int) Unique ID of the cluster. Must be unique across all connected
    # clusters and in the range of 1 to 255. Only required for Cluster Mesh,
    # may be 0 if Cluster Mesh is not used.
    id: 0
  # -- Define serviceAccount names for components.
  # @default -- Component's fully qualified name.
  serviceAccounts:
    cilium:
      create: true
      name: cilium
      automount: true
      annotations: {}
    nodeinit:
      create: true
      # -- Enabled is temporary until https://github.com/cilium/cilium-cli/issues/1396 is implemented.
      # Cilium CLI doesn't create the SAs for node-init, thus the workaround. Helm is not affected by
      # this issue. Name and automount can be configured, if enabled is set to true.
      # Otherwise, they are ignored. Enabled can be removed once the issue is fixed.
      # Cilium-nodeinit DS must also be fixed.
      enabled: false
      name: cilium-nodeinit
      automount: true
      annotations: {}
    envoy:
      create: true
      name: cilium-envoy
      automount: true
      annotations: {}
    operator:
      create: true
      name: cilium-operator
      automount: true
      annotations: {}
    preflight:
      create: true
      name: cilium-pre-flight
      automount: true
      annotations: {}
    relay:
      create: true
      name: hubble-relay
      automount: false
      annotations: {}
    ui:
      create: true
      name: hubble-ui
      automount: true
      annotations: {}
    clustermeshApiserver:
      create: true
      name: clustermesh-apiserver
      automount: true
      annotations: {}
    # -- Clustermeshcertgen is used if clustermesh.apiserver.tls.auto.method=cronJob
    clustermeshcertgen:
      create: true
      name: clustermesh-apiserver-generate-certs
      automount: true
      annotations: {}
    # -- Hubblecertgen is used if hubble.tls.auto.method=cronJob
    hubblecertgen:
      create: true
      name: hubble-generate-certs
      automount: true
      annotations: {}
  # -- Configure termination grace period for cilium-agent DaemonSet.
  terminationGracePeriodSeconds: 1
  # -- Install the cilium agent resources.
  agent: true
  # -- Agent container name.
  name: cilium
  # -- Roll out cilium agent pods automatically when configmap is updated.
  rollOutCiliumPods: true
  # -- Agent container image.
  image:
    # @schema
    # type: [null, string]
    # @schema
    override: ~
    repository: "quay.io/cilium/cilium"
    pullPolicy: "IfNotPresent"
  # -- Scheduling configurations for cilium pods
  scheduling:
    # @schema
    # enum: ["anti-affinity", "kube-scheduler"]
    # @schema
    # -- Mode specifies how Cilium daemonset pods should be scheduled to Nodes.
    # `anti-affinity` mode applies a pod anti-affinity rule to the cilium daemonset.
    # Pod anti-affinity may significantly impact scheduling throughput for large clusters.
    # See: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    # `kube-scheduler` mode forgoes the anti-affinity rule for full scheduling throughput.
    # Kube-scheduler avoids host port conflict when scheduling pods.
    # @default -- Defaults to apply a pod anti-affinity rule to the agent pod - `anti-affinity`
    mode: anti-affinity
  # -- Affinity for cilium-agent.
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              k8s-app: cilium
  # -- Node selector for cilium-agent.
  nodeSelector:
    kubernetes.io/os: linux
  # -- Node tolerations for agent scheduling to nodes with taints
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations:
    - operator: Exists
      # - key: "key"
      #   operator: "Equal|Exists"
      #   value: "value"
      #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"
  # -- The priority class to use for cilium-agent.
  priorityClassName: system-node-critical
  # -- DNS policy for Cilium agent pods.
  # Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
  dnsPolicy: ""
  # -- Additional containers added to the cilium DaemonSet.
  extraContainers: []
  # -- Additional initContainers added to the cilium Daemonset.
  extraInitContainers: []
  # -- Additional agent container arguments.
  extraArgs: []
  # -- Additional agent container environment variables.
  extraEnv: []
  # -- Additional agent hostPath mounts.
  extraHostPathMounts: []
  # - name: host-mnt-data
  #   mountPath: /host/mnt/data
  #   hostPath: /mnt/data
  #   hostPathType: Directory
  #   readOnly: true
  #   mountPropagation: HostToContainer

  # -- Additional agent volumes.
  extraVolumes: []
  # -- Additional agent volumeMounts.
  extraVolumeMounts: []
  # -- extraConfig allows you to specify additional configuration parameters to be
  # included in the cilium-config configmap.
  extraConfig:
    bpf-ct-timeout-regular-any: 1h0m0s
    bpf-ct-timeout-service-any: 1h0m0s
    hubble-tls-client-ca-files: /var/lib/cilium/tls/hubble/server.crt
  #  my-config-a: "1234"
  #  my-config-b: |-
  #    test 1
  #    test 2
  #    test 3

  # -- Annotations to be added to all top-level cilium-agent objects (resources under templates/cilium-agent)
  annotations: {}
  # -- Security Context for cilium-agent pods.
  podSecurityContext:
    # -- AppArmorProfile options for the `cilium-agent` and init containers
    appArmorProfile:
      type: 'Unconfined'

  # -- Annotations to be added to agent pods
  podAnnotations: {}

  # -- Labels to be added to agent pods
  podLabels:
    logs: loki-myloginspace


  # -- Agent resource limits & requests
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources:
    limits:
      cpu: 4000m
      memory: 4Gi
    requests:
      cpu: 100m
      memory: 512Mi

  # -- resources & limits for the agent init containers
  initResources: {}
  securityContext:
    # -- User to run the pod with
    # runAsUser: 0
    # -- Run the pod with elevated privileges
    privileged: false
    # -- SELinux options for the `cilium-agent` and init containers
    seLinuxOptions:
      level: 's0'
      # Running with spc_t since we have removed the privileged mode.
      # Users can change it to a different type as long as they have the
      # type available on the system.
      type: 'spc_t'
    capabilities:
      # -- Capabilities for the `cilium-agent` container
      ciliumAgent:
        # Use to set socket permission
        - CHOWN
        # Used to terminate envoy child process
        - KILL
        # Used since cilium modifies routing tables, etc...
        - NET_ADMIN
        # Used since cilium creates raw sockets, etc...
        - NET_RAW
        # Used since cilium monitor uses mmap
        - IPC_LOCK
        # Used in iptables. Consider removing once we are iptables-free
        - SYS_MODULE
        # Needed to switch network namespaces (used for health endpoint, socket-LB).
        # We need it for now but might not need it for >= 5.11 specially
        # for the 'SYS_RESOURCE'.
        # In >= 5.8 there's already BPF and PERMON capabilities
        - SYS_ADMIN
        # Could be an alternative for the SYS_ADMIN for the RLIMIT_NPROC
        - SYS_RESOURCE
        # Both PERFMON and BPF requires kernel 5.8, container runtime
        # cri-o >= v1.22.0 or containerd >= v1.5.0.
        # If available, SYS_ADMIN can be removed.
        #- PERFMON
        #- BPF
        # Allow discretionary access control (e.g. required for package installation)
        - DAC_OVERRIDE
        # Allow to set Access Control Lists (ACLs) on arbitrary files (e.g. required for package installation)
        - FOWNER
        # Allow to execute program that changes GID (e.g. required for package installation)
        - SETGID
        # Allow to execute program that changes UID (e.g. required for package installation)
        - SETUID
      # -- Capabilities for the `mount-cgroup` init container
      mountCgroup:
        # Only used for 'mount' cgroup
        - SYS_ADMIN
        # Used for nsenter
        - SYS_CHROOT
        - SYS_PTRACE
      # -- capabilities for the `apply-sysctl-overwrites` init container
      applySysctlOverwrites:
        # Required in order to access host's /etc/sysctl.d dir
        - SYS_ADMIN
        # Used for nsenter
        - SYS_CHROOT
        - SYS_PTRACE
      # -- Capabilities for the `clean-cilium-state` init container
      cleanCiliumState:
        # Most of the capabilities here are the same ones used in the
        # cilium-agent's container because this container can be used to
        # uninstall all Cilium resources, and therefore it is likely that
        # will need the same capabilities.
        # Used since cilium modifies routing tables, etc...
        - NET_ADMIN
        # Used in iptables. Consider removing once we are iptables-free
        - SYS_MODULE
        # We need it for now but might not need it for >= 5.11 specially
        # for the 'SYS_RESOURCE'.
        # In >= 5.8 there's already BPF and PERMON capabilities
        - SYS_ADMIN
        # Could be an alternative for the SYS_ADMIN for the RLIMIT_NPROC
        - SYS_RESOURCE
        # Both PERFMON and BPF requires kernel 5.8, container runtime
        # cri-o >= v1.22.0 or containerd >= v1.5.0.
        # If available, SYS_ADMIN can be removed.
        #- PERFMON
        #- BPF
  # -- Cilium agent update strategy
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      # @schema
      # type: [integer, string]
      # @schema
      maxUnavailable: 2
  # Configuration Values for cilium-agent
  aksbyocni:
    # -- Enable AKS BYOCNI integration.
    # Note that this is incompatible with AKS clusters not created in BYOCNI mode:
    # use Azure integration (`azure.enabled`) instead.
    enabled: false
  # @schema
  # type: [boolean, string]
  # @schema
  # -- Enable installation of PodCIDR routes between worker
  # nodes if worker nodes share a common L2 network segment.
  autoDirectNodeRoutes: false
  # -- Enable skipping of PodCIDR routes between worker
  # nodes if the worker nodes are in a different L2 network segment.
  directRoutingSkipUnreachable: false
  # -- Annotate k8s node upon initialization with Cilium's metadata.
  annotateK8sNode: false
  azure:
    # -- Enable Azure integration.
    # Note that this is incompatible with AKS clusters created in BYOCNI mode: use
    # AKS BYOCNI integration (`aksbyocni.enabled`) instead.
    enabled: false
    # usePrimaryAddress: false
    # resourceGroup: group1
    # subscriptionID: 00000000-0000-0000-0000-000000000000
    # tenantID: 00000000-0000-0000-0000-000000000000
    # clientID: 00000000-0000-0000-0000-000000000000
    # clientSecret: 00000000-0000-0000-0000-000000000000
    # userAssignedIdentityID: 00000000-0000-0000-0000-000000000000
  alibabacloud:
    # -- Enable AlibabaCloud ENI integration
    enabled: false
  # -- Enable bandwidth manager to optimize TCP and UDP workloads and allow
  # for rate-limiting traffic from individual Pods with EDT (Earliest Departure
  # Time) through the "kubernetes.io/egress-bandwidth" Pod annotation.
  bandwidthManager:
    # -- Enable bandwidth manager infrastructure (also prerequirement for BBR)
    enabled: true
    # -- Activate BBR TCP congestion control for Pods
    bbr: true

  # -- Configure standalone NAT46/NAT64 gateway
  nat46x64Gateway:
    # -- Enable RFC8215-prefixed translation
    enabled: false
  # -- EnableHighScaleIPcache enables the special ipcache mode for high scale
  # clusters. The ipcache content will be reduced to the strict minimum and
  # traffic will be encapsulated to carry security identities.
  highScaleIPcache:
    # -- Enable the high scale mode for the ipcache.
    enabled: false
  # -- Configure L2 announcements
  l2announcements:
    # -- Enable L2 announcements
    enabled: false
    # -- If a lease is not renewed for X duration, the current leader is considered dead, a new leader is picked
    # leaseDuration: 15s
    # -- The interval at which the leader will renew the lease
    # leaseRenewDeadline: 5s
    # -- The timeout between retries if renewal fails
    # leaseRetryPeriod: 2s
  # -- Configure L2 pod announcements
  l2podAnnouncements:
    # -- Enable L2 pod announcements
    enabled: false
    # -- Interface used for sending Gratuitous ARP pod announcements
    interface: "eth0"

  # -- This feature set enables virtual BGP routers to be created via
  # CiliumBGPPeeringPolicy CRDs.
  bgpControlPlane:
    # -- Enables the BGP control plane.
    enabled: true
    # -- SecretsNamespace is the namespace which BGP support will retrieve secrets from.
    secretsNamespace:
      # -- Create secrets namespace for BGP secrets.
      create: false
      # -- The name of the secret namespace to which Cilium agents are given read access
      name: kube-system

    # -- Status reporting settings (BGPv2 only)
    statusReport:
      # -- Enable/Disable BGPv2 status reporting
      # It is recommended to enable status reporting in general, but if you have any issue
      # such as high API server load, you can disable it by setting this to false.
      enabled: true

  pmtuDiscovery:
    # -- Enable path MTU discovery to send ICMP fragmentation-needed replies to
    # the client.
    enabled: true

  bpf:
    autoMount:
      # -- Enable automatic mount of BPF filesystem
      # When `autoMount` is enabled, the BPF filesystem is mounted at
      # `bpf.root` path on the underlying host and inside the cilium agent pod.
      # If users disable `autoMount`, it's expected that users have mounted
      # bpffs filesystem at the specified `bpf.root` volume, and then the
      # volume will be mounted inside the cilium agent pod at the same path.
      enabled: true
    # -- Configure the mount point for the BPF filesystem
    root: /sys/fs/bpf
    # -- Enables pre-allocation of eBPF map values. This increases
    # memory usage but can reduce latency.
    preallocateMaps: false
    # @schema
    # type: [null, integer]
    # @schema
    # -- (int) Configure the maximum number of entries in auth map.
    # @default -- `524288`
    authMapMax: ~
    # -- Enable CT accounting for packets and bytes
    ctAccounting: false
    # @schema
    # type: [null, integer]
    # @schema
    # -- (int) Configure the maximum number of entries in the TCP connection tracking
    # table.
    # @default -- `524288`
    ctTcpMax: ~
    # @schema
    # type: [null, integer]
    # @schema
    # -- (int) Configure the maximum number of entries for the non-TCP connection
    # tracking table.
    # @default -- `262144`
    ctAnyMax: ~
    # -- Control events generated by the Cilium datapath exposed to Cilium monitor and Hubble.
    # Helm configuration for BPF events map rate limiting is experimental and might change
    # in upcoming releases.
    events:
      # -- Default settings for all types of events except dbg and pcap.
      default:
        # -- (int) Configure the limit of messages per second that can be written to
        # BPF events map. The number of messages is averaged, meaning that if no messages
        # were written to the map over 5 seconds, it's possible to write more events
        # in the 6th second. If rateLimit is greater than 0, non-zero value for burstLimit must
        # also be provided lest the configuration is considered invalid. Setting both burstLimit
        # and rateLimit to 0 disables BPF events rate limiting.
        # @default -- `0`
        rateLimit: ~
        # -- (int) Configure the maximum number of messages that can be written to BPF events
        # map in 1 second. If burstLimit is greater than 0, non-zero value for rateLimit must
        # also be provided lest the configuration is considered invalid. Setting both burstLimit
        # and rateLimit to 0 disables BPF events rate limiting.
        # @default -- `0`
        burstLimit: ~
      drop:
        # -- Enable drop events.
        enabled: true
      policyVerdict:
        # -- Enable policy verdict events.
        enabled: true
      trace:
        # -- Enable trace events.
        enabled: true
    # @schema
    # type: [null, integer]
    # @schema
    # -- Configure the maximum number of service entries in the
    # load balancer maps.
    lbMapMax: 65536
    # @schema
    # type: [null, integer]
    # @schema
    # -- (int) Configure the maximum number of entries for the NAT table.
    # @default -- `524288`
    natMax: null
    # @schema
    # type: [null, integer]
    # @schema
    # -- (int) Configure the maximum number of entries for the neighbor table.
    # @default -- `524288`
    neighMax: null
    # @schema
    # type: [null, integer]
    # @schema
    # @default -- `16384`
    # -- (int) Configures the maximum number of entries for the node table.
    nodeMapMax: ~
    # -- Configure the maximum number of entries in endpoint policy map (per endpoint).
    # @schema
    # type: [null, integer]
    # @schema
    policyMapMax: 65536

    # @schema
    # type: [null, number]
    # @schema
    # -- (float64) Configure auto-sizing for all BPF maps based on available memory.
    # ref: https://docs.cilium.io/en/stable/network/ebpf/maps/
    # @default -- `0.0025`
    mapDynamicSizeRatio: 0.005

    # -- Configure the level of aggregation for monitor notifications.
    # Valid options are none, low, medium, maximum.
    monitorAggregation: medium

    # -- Configure the typical time between monitor notifications for
    # active connections.
    monitorInterval: "5s"

    # -- Configure which TCP flags trigger notifications when seen for the
    # first time in a connection.
    monitorFlags: "all"

    # -- (bool) Allow cluster external access to ClusterIP services.
    # @default -- `false`
    lbExternalClusterIP: true

    # -- (bool) Enable loadBalancerSourceRanges CIDR filtering for all service
    # types, not just LoadBalancer services. The corresponding NodePort and
    # ClusterIP (if enabled for cluster-external traffic) will also apply the
    # CIDR filter.
    # @default -- `false`
    lbSourceRangeAllTypes: true

    # -- (bool) Enable the option to define the load balancing algorithm on
    # a per-service basis through service.cilium.io/lb-algorithm annotation.
    # @default -- `false`
    lbAlgorithmAnnotation: false
    # -- (bool) Enable the option to define the load balancing mode (SNAT or DSR)
    # on a per-service basis through service.cilium.io/forwarding-mode annotation.
    # @default -- `false`
    lbModeAnnotation: false
    # @schema
    # type: [null, boolean]
    # @schema
    # -- (bool) Enable native IP masquerade support in eBPF
    # @default -- `false`
    masquerade: true
    # @schema
    # type: [null, boolean]
    # @schema
    # -- (bool) Configure whether direct routing mode should route traffic via
    # host stack (true) or directly and more efficiently out of BPF (false) if
    # the kernel supports it. The latter has the implication that it will also
    # bypass netfilter in the host namespace.
    # @default -- `false`
    hostLegacyRouting: false
    # @schema
    # type: [null, boolean]
    # @schema
    # -- (bool) Configure the eBPF-based TPROXY (beta) to reduce reliance on iptables rules
    # for implementing Layer 7 policy.
    # @default -- `false`
    tproxy: true
    # @schema
    # type: [null, array]
    # @schema
    # -- (list) Configure explicitly allowed VLAN id's for bpf logic bypass.
    # [0] will allow all VLAN id's without any filtering.
    # @default -- `[]`
    vlanBypass: ~
    # -- (bool) Disable ExternalIP mitigation (CVE-2020-8554)
    # @default -- `false`
    disableExternalIPMitigation: false
    # -- (bool) Attach endpoint programs using tcx instead of legacy tc hooks on
    # supported kernels.
    # @default -- `true`
    enableTCX: true
    # -- (string) Mode for Pod devices for the core datapath (veth, netkit, netkit-l2, lb-only)
    # @default -- `veth`
    datapathMode: veth
  # -- Enable BPF clock source probing for more efficient tick retrieval.
  bpfClockProbe: false
  # -- Clean all eBPF datapath state from the initContainer of the cilium-agent
  # DaemonSet.
  #
  # WARNING: Use with care!
  cleanBpfState: false
  # -- Clean all local Cilium state from the initContainer of the cilium-agent
  # DaemonSet. Implies cleanBpfState: true.
  #
  # WARNING: Use with care!
  cleanState: false
  # -- Wait for KUBE-PROXY-CANARY iptables rule to appear in "wait-for-kube-proxy"
  # init container before launching cilium-agent.
  # More context can be found in the commit message of below PR
  # https://github.com/cilium/cilium/pull/20123
  waitForKubeProxy: false
  cni:
    # -- Install the CNI configuration and binary files into the filesystem.
    install: true
    # -- Remove the CNI configuration and binary files on agent shutdown. Enable this
    # if you're removing Cilium from the cluster. Disable this to prevent the CNI
    # configuration file from being removed during agent upgrade, which can cause
    # nodes to go unmanageable.
    uninstall: false
    # @schema
    # type: [null, string]
    # @schema
    # -- Configure chaining on top of other CNI plugins. Possible values:
    #  - none
    #  - aws-cni
    #  - flannel
    #  - generic-veth
    #  - portmap
    chainingMode: ~
    # @schema
    # type: [null, string]
    # @schema
    # -- A CNI network name in to which the Cilium plugin should be added as a chained plugin.
    # This will cause the agent to watch for a CNI network with this network name. When it is
    # found, this will be used as the basis for Cilium's CNI configuration file. If this is
    # set, it assumes a chaining mode of generic-veth. As a special case, a chaining mode
    # of aws-cni implies a chainingTarget of aws-cni.
    chainingTarget: ~
    # -- Make Cilium take ownership over the `/etc/cni/net.d` directory on the
    # node, renaming all non-Cilium CNI configurations to `*.cilium_bak`.
    # This ensures no Pods can be scheduled using other CNI plugins during Cilium
    # agent downtime.
    exclusive: true
    # -- Configure the log file for CNI logging with retention policy of 7 days.
    # Disable CNI file logging by setting this field to empty explicitly.
    logFile: /var/run/cilium/cilium-cni.log
    # -- Skip writing of the CNI configuration. This can be used if
    # writing of the CNI configuration is performed by external automation.
    customConf: false
    # -- Configure the path to the CNI configuration directory on the host.
    confPath: /etc/cni/net.d
    # -- Configure the path to the CNI binary directory on the host.
    binPath: /opt/cni/bin
    # -- Specify the path to a CNI config to read from on agent start.
    # This can be useful if you want to manage your CNI
    # configuration outside of a Kubernetes environment. This parameter is
    # mutually exclusive with the 'cni.configMap' parameter. The agent will
    # write this to 05-cilium.conflist on startup.
    # readCniConf: /host/etc/cni/net.d/05-sample.conflist.input

    # -- When defined, configMap will mount the provided value as ConfigMap and
    # interpret the cniConf variable as CNI configuration file and write it
    # when the agent starts up
    # configMap: cni-configuration

    # -- Configure the key in the CNI ConfigMap to read the contents of
    # the CNI configuration from.
    configMapKey: cni-config
    # -- Configure the path to where to mount the ConfigMap inside the agent pod.
    confFileMountPath: /tmp/cni-configuration
    # -- Configure the path to where the CNI configuration directory is mounted
    # inside the agent pod.
    hostConfDirMountPath: /host/etc/cni/net.d
    # -- Specifies the resources for the cni initContainer
    resources:
      requests:
        cpu: 100m
        memory: 10Mi
    # -- Enable route MTU for pod netns when CNI chaining is used
    enableRouteMTUForCNIChaining: true
  # -- (string) Configure how frequently garbage collection should occur for the datapath
  # connection tracking table.
  # @default -- `"0s"`
  conntrackGCInterval: ""
  # -- (string) Configure the maximum frequency for the garbage collection of the
  # connection tracking table. Only affects the automatic computation for the frequency
  # and has no effect when 'conntrackGCInterval' is set. This can be set to more frequently
  # clean up unused identities created from ToFQDN policies.
  conntrackGCMaxInterval: '30m'
  # -- (string) Configure timeout in which Cilium will exit if CRDs are not available
  # @default -- `"5m"`
  crdWaitTimeout: ""
  # -- Tail call hooks for custom eBPF programs.
  customCalls:
    # -- Enable tail call hooks for custom eBPF programs.
    enabled: false

  daemon:
    # -- Configure where Cilium runtime state should be stored.
    runPath: "/var/run/cilium"
    # @schema
    # type: [null, string]
    # @schema
    # -- Configure a custom list of possible configuration override sources
    # The default is "config-map:cilium-config,cilium-node-config". For supported
    # values, see the help text for the build-config subcommand.
    # Note that this value should be a comma-separated string.
    configSources: ~
    # @schema
    # type: [null, string]
    # @schema
    # -- allowedConfigOverrides is a list of config-map keys that can be overridden.
    # That is to say, if this value is set, config sources (excepting the first one) can
    # only override keys in this list.
    #
    # This takes precedence over blockedConfigOverrides.
    #
    # By default, all keys may be overridden. To disable overrides, set this to "none" or
    # change the configSources variable.
    allowedConfigOverrides: ~
    # @schema
    # type: [null, string]
    # @schema
    # -- blockedConfigOverrides is a list of config-map keys that may not be overridden.
    # In other words, if any of these keys appear in a configuration source excepting the
    # first one, they will be ignored
    #
    # This is ignored if allowedConfigOverrides is set.
    #
    # By default, all keys may be overridden.
    blockedConfigOverrides: ~
    # @schema
    # type: [null, boolean]
    # @schema
    # -- enableSourceIPVerification is a boolean flag to enable or disable the Source IP verification
    # of endpoints. This flag is useful when Cilium is chained with other CNIs.
    #
    # By default, this functionality is enabled
    enableSourceIPVerification: false
  # -- Specify which network interfaces can run the eBPF datapath. This means
  # that a packet sent from a pod to a destination outside the cluster will be
  # masqueraded (to an output device IPv4 address), if the output device runs the
  # program. When not specified, probing will automatically detect devices that have
  # a non-local route. This should be used only when autodetection is not suitable.
  devices: "eno1,wg-0,wg-1"

  # -- Forces the auto-detection of devices, even if specific devices are explicitly listed
  forceDeviceDetection: false
  # -- Chains to ignore when installing feeder rules.
  # disableIptablesFeederRules: ""

  # -- Limit iptables-based egress masquerading to interface selector.
  # egressMasqueradeInterfaces: ""

  # -- Enable setting identity mark for local traffic.
  # enableIdentityMark: true

  # -- Enable Kubernetes EndpointSlice feature in Cilium if the cluster supports it.
  # enableK8sEndpointSlice: true

  # -- Enable CiliumEndpointSlice feature (deprecated, please use `ciliumEndpointSlice.enabled` instead).
  enableCiliumEndpointSlice: false
  ciliumEndpointSlice:
    # -- Enable Cilium EndpointSlice feature.
    enabled: false

  envoyConfig:
    # -- Enable CiliumEnvoyConfig CRD
    # CiliumEnvoyConfig CRD can also be implicitly enabled by other options.
    enabled: false

  ingressController:
    # -- Enable cilium ingress controller
    # This will automatically set enable-envoy-config as well.
    enabled: false
    # -- Set cilium ingress controller to be the default ingress controller
    # This will let cilium ingress controller route entries without ingress class set
    default: false
    # -- Default ingress load balancer mode
    # Supported values: shared, dedicated
    # For granular control, use the following annotations on the ingress resource:
    # "ingress.cilium.io/loadbalancer-mode: dedicated" (or "shared").
    loadbalancerMode: dedicated
    # -- Enforce https for host having matching TLS host in Ingress.
    # Incoming traffic to http listener will return 308 http error code with respective location in header.
    enforceHttps: true
    # -- Enable proxy protocol for all Ingress listeners. Note that _only_ Proxy protocol traffic will be accepted once this is enabled.
    enableProxyProtocol: false
    # -- IngressLBAnnotations are the annotation and label prefixes, which are used to filter annotations and/or labels to propagate from Ingress to the Load Balancer service
    ingressLBAnnotationPrefixes: ['lbipam.cilium.io', 'nodeipam.cilium.io', 'service.beta.kubernetes.io', 'service.kubernetes.io', 'cloud.google.com']
    # @schema
    # type: [null, string]
    # @schema
    # -- Default secret namespace for ingresses without .spec.tls[].secretName set.
    defaultSecretNamespace:
    # @schema
    # type: [null, string]
    # @schema
    # -- Default secret name for ingresses without .spec.tls[].secretName set.
    defaultSecretName:
    # -- SecretsNamespace is the namespace in which envoy SDS will retrieve TLS secrets from.
    secretsNamespace:
      # -- Create secrets namespace for Ingress.
      create: true
      # -- Name of Ingress secret namespace.
      name: cilium-secrets
      # -- Enable secret sync, which will make sure all TLS secrets used by Ingress are synced to secretsNamespace.name.
      # If disabled, TLS secrets must be maintained externally.
      sync: true
    # -- Load-balancer service in shared mode.
    # This is a single load-balancer service for all Ingress resources.
    service:
      # -- Service name
      name: cilium-ingress
      # -- Labels to be added for the shared LB service
      labels: {}
      # -- Annotations to be added for the shared LB service
      annotations: {}
      # -- Service type for the shared LB service
      type: LoadBalancer
      # @schema
      # type: [null, integer]
      # @schema
      # -- Configure a specific nodePort for insecure HTTP traffic on the shared LB service
      insecureNodePort: ~
      # @schema
      # type: [null, integer]
      # @schema
      # -- Configure a specific nodePort for secure HTTPS traffic on the shared LB service
      secureNodePort: ~
      # @schema
      # type: [null, string]
      # @schema
      # -- Configure a specific loadBalancerClass on the shared LB service (requires Kubernetes 1.24+)
      loadBalancerClass: ~
      # @schema
      # type: [null, string]
      # @schema
      # -- Configure a specific loadBalancerIP on the shared LB service
      loadBalancerIP: ~
      # @schema
      # type: [null, boolean]
      # @schema
      # -- Configure if node port allocation is required for LB service
      # ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation
      allocateLoadBalancerNodePorts: ~
      # -- Control how traffic from external sources is routed to the LoadBalancer Kubernetes Service for Cilium Ingress in shared mode.
      # Valid values are "Cluster" and "Local".
      # ref: https://kubernetes.io/docs/reference/networking/virtual-ips/#external-traffic-policy
      externalTrafficPolicy: Cluster
    # Host Network related configuration
    hostNetwork:
      # -- Configure whether the Envoy listeners should be exposed on the host network.
      enabled: false
      # -- Configure a specific port on the host network that gets used for the shared listener.
      sharedListenerPort: 8080
      # Specify the nodes where the Ingress listeners should be exposed
      nodes:
        # -- Specify the labels of the nodes where the Ingress listeners should be exposed
        #
        # matchLabels:
        #   kubernetes.io/os: linux
        #   kubernetes.io/hostname: kind-worker
        matchLabels: {}

  gatewayAPI:
    # -- Enable support for Gateway API in cilium
    # This will automatically set enable-envoy-config as well.
    enabled: false
    # -- Enable proxy protocol for all GatewayAPI listeners. Note that _only_ Proxy protocol traffic will be accepted once this is enabled.
    enableProxyProtocol: false
    # -- Enable Backend Protocol selection support (GEP-1911) for Gateway API via appProtocol.
    enableAppProtocol: false
    # -- Enable ALPN for all listeners configured with Gateway API. ALPN will attempt HTTP/2, then HTTP 1.1.
    # Note that this will also enable `appProtocol` support, and services that wish to use HTTP/2 will need to indicate that via their `appProtocol`.
    enableAlpn: false
    # -- The number of additional GatewayAPI proxy hops from the right side of the HTTP header to trust when determining the origin client's IP address.
    xffNumTrustedHops: 0
    # -- Control how traffic from external sources is routed to the LoadBalancer Kubernetes Service for all Cilium GatewayAPI Gateway instances. Valid values are "Cluster" and "Local".
    # Note that this value will be ignored when `hostNetwork.enabled == true`.
    # ref: https://kubernetes.io/docs/reference/networking/virtual-ips/#external-traffic-policy
    externalTrafficPolicy: Cluster
    gatewayClass:
      # -- Enable creation of GatewayClass resource
      # The default value is 'auto' which decides according to presence of gateway.networking.k8s.io/v1/GatewayClass in the cluster.
      # Other possible values are 'true' and 'false', which will either always or never create the GatewayClass, respectively.
      create: auto
    # -- SecretsNamespace is the namespace in which envoy SDS will retrieve TLS secrets from.
    secretsNamespace:
      # -- Create secrets namespace for Gateway API.
      create: true
      # -- Name of Gateway API secret namespace.
      name: cilium-secrets
      # -- Enable secret sync, which will make sure all TLS secrets used by Ingress are synced to secretsNamespace.name.
      # If disabled, TLS secrets must be maintained externally.
      sync: true
    # Host Network related configuration
    hostNetwork:
      # -- Configure whether the Envoy listeners should be exposed on the host network.
      enabled: false
      # Specify the nodes where the Ingress listeners should be exposed
      nodes:
        # -- Specify the labels of the nodes where the Ingress listeners should be exposed
        #
        # matchLabels:
        #   kubernetes.io/os: linux
        #   kubernetes.io/hostname: kind-worker
        matchLabels: {}

  # -- Enables the fallback compatibility solution for when the xt_socket kernel
  # module is missing and it is needed for the datapath L7 redirection to work
  # properly. See documentation for details on when this can be disabled:
  # https://docs.cilium.io/en/stable/operations/system_requirements/#linux-kernel.
  enableXTSocketFallback: true

  encryption:
    # -- Enable transparent network encryption.
    enabled: false
    # -- Encryption method. Can be either ipsec or wireguard.
    type: ipsec
    # -- Enable encryption for pure node to node traffic.
    # This option is only effective when encryption.type is set to "wireguard".
    nodeEncryption: false
    # -- Configure the WireGuard Pod2Pod strict mode.
    strictMode:
      # -- Enable WireGuard Pod2Pod strict mode.
      enabled: false
      # -- CIDR for the WireGuard Pod2Pod strict mode.
      cidr: ""
      # -- Allow dynamic lookup of remote node identities.
      # This is required when tunneling is used or direct routing is used and the node CIDR and pod CIDR overlap.
      allowRemoteNodeIdentities: false
    ipsec:
      # -- Name of the key file inside the Kubernetes secret configured via secretName.
      keyFile: keys
      # -- Path to mount the secret inside the Cilium pod.
      mountPath: /etc/ipsec
      # -- Name of the Kubernetes secret containing the encryption keys.
      secretName: cilium-ipsec-keys
      # -- The interface to use for encrypted traffic.
      interface: ""
      # -- Enable the key watcher. If disabled, a restart of the agent will be
      # necessary on key rotations.
      keyWatcher: true
      # -- Maximum duration of the IPsec key rotation. The previous key will be
      # removed after that delay.
      keyRotationDuration: "5m"
      # -- Enable IPsec encrypted overlay
      encryptedOverlay: false
    wireguard:
      # -- Controls WireGuard PersistentKeepalive option. Set 0s to disable.
      persistentKeepalive: 0s
  endpointHealthChecking:
    # -- Enable connectivity health checking between virtual endpoints.
    enabled: true
  endpointRoutes:
    # @schema
    # type: [boolean, string]
    # @schema
    # -- Enable use of per endpoint routes instead of routing via
    # the cilium_host interface.
    enabled: true

  k8sNetworkPolicy:
    # -- Enable support for K8s NetworkPolicy
    enabled: true

  # -- Enable endpoint lockdown on policy map overflow.
  endpointLockdownOnMapOverflow: false
  
  externalIPs:
    # -- Enable ExternalIPs service support.
    enabled: true
  
  # fragmentTracking enables IPv4 fragment tracking support in the datapath.
  fragmentTracking: true
  
  # -- Enable connectivity health checking.
  healthChecking: true
  
  # -- TCP port for the agent health API. This is not the port for cilium-health.
  healthPort: 9879
  
  # -- Number of ICMP requests sent for each health check before marking a node or endpoint unreachable.
  healthCheckICMPFailureThreshold: 3

  # -- Configure the host firewall.
  hostFirewall:
    # -- Enables the enforcement of host policies in the eBPF datapath.
    enabled: false

  hostPort:
    # -- Enable hostPort service support.
    enabled: false

  # -- Configure socket LB
  socketLB:
    # -- Enable socket LB
    enabled: false    
    # -- Disable socket lb for non-root ns. This is used to enable Istio routing rules.
    # hostNamespaceOnly: false
    # -- Enable terminating pod connections to deleted service backends.
    # terminatePodConnections: true
    # -- Enables tracing for socket-based load balancing.
    # tracing: true

  # -- Configure certificate generation for Hubble integration.
  # If hubble.tls.auto.method=cronJob, these values are used
  # for the Kubernetes CronJob which will be scheduled regularly to
  # (re)generate any certificates not provided manually.
  certgen:
    # -- When set to true the certificate authority secret is created.
    generateCA: true

    # -- Seconds after which the completed job pod will be deleted
    ttlSecondsAfterFinished: 1800
    # -- Labels to be added to hubble-certgen pods
    podLabels: {}
    # -- Annotations to be added to the hubble-certgen initial Job and CronJob
    annotations:
      job: {}
      cronJob: {}
    # -- Node selector for certgen
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    nodeSelector: {}
    # -- Priority class for certgen
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass
    priorityClassName: ""
    # -- Node tolerations for pod assignment on nodes with taints
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    tolerations: []
    # -- Additional certgen volumes.
    extraVolumes: []
    # -- Additional certgen volumeMounts.
    extraVolumeMounts: []
    # -- Affinity for certgen
    affinity: {}
  hubble:
    # -- Enable Hubble (true by default).
    enabled: true

    # -- Buffer size of the channel Hubble uses to receive monitor events. If this
    # value is not set, the queue size is set to the default monitor queue size.
    eventQueueSize: 32768

    # -- Number of recent flows for Hubble to cache. Defaults to 4095.
    # Possible values are:
    #   1, 3, 7, 15, 31, 63, 127, 255, 511, 1023,
    #   2047, 4095, 8191, 16383, 32767, 65535
    # eventBufferCapacity: "4095"

    # -- Hubble metrics configuration.
    # See https://docs.cilium.io/en/stable/observability/metrics/#hubble-metrics
    # for more comprehensive documentation about Hubble metrics.
    metrics:
      # @schema
      # type: [null, array]
      # @schema
      # -- Configures the list of metrics to collect. If empty or null, metrics
      # are disabled.
      # Example:
      #
      #   enabled:
      #   - dns:query;ignoreAAAA
      #   - drop
      #   - tcp
      #   - flow
      #   - icmp
      #   - http
      #
      # You can specify the list of metrics from the helm CLI:
      #
      #   --set hubble.metrics.enabled="{dns:query;ignoreAAAA,drop,tcp,flow,icmp,http}"
      #
      enabled: null
      # -- Enables exporting hubble metrics in OpenMetrics format.
      enableOpenMetrics: false
      # -- Configure the port the hubble metric server listens on.
      port: 9965
      tls:
        # Enable hubble metrics server TLS.
        enabled: false
        # Configure hubble metrics server TLS.
        server:
          # -- Name of the Secret containing the certificate and key for the Hubble metrics server.
          # If specified, cert and key are ignored.
          existingSecret: ""
          # -- base64 encoded PEM values for the Hubble metrics server certificate (deprecated).
          # Use existingSecret instead.
          cert: ""
          # -- base64 encoded PEM values for the Hubble metrics server key (deprecated).
          # Use existingSecret instead.
          key: ""
          # -- Extra DNS names added to certificate when it's auto generated
          extraDnsNames: []
          # -- Extra IP addresses added to certificate when it's auto generated
          extraIpAddresses: []
          # -- Configure mTLS for the Hubble metrics server.
          mtls:
            # When set to true enforces mutual TLS between Hubble Metrics server and its clients.
            # False allow non-mutual TLS connections.
            # This option has no effect when TLS is disabled.
            enabled: false
            useSecret: false
            # -- Name of the ConfigMap containing the CA to validate client certificates against.
            # If mTLS is enabled and this is unspecified, it will default to the
            # same CA used for Hubble metrics server certificates.
            name: ~
            # -- Entry of the ConfigMap containing the CA.
            key: ca.crt
      # -- Annotations to be added to hubble-metrics service.
      serviceAnnotations: {}
      serviceMonitor:
        # -- Create ServiceMonitor resources for Prometheus Operator.
        # This requires the prometheus CRDs to be available.
        # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
        enabled: false
        # -- Labels to add to ServiceMonitor hubble
        labels: {}
        # -- Annotations to add to ServiceMonitor hubble
        annotations: {}
        # -- jobLabel to add for ServiceMonitor hubble
        jobLabel: ""
        # -- Interval for scrape metrics.
        interval: "10s"
        # -- Relabeling configs for the ServiceMonitor hubble
        relabelings:
          - sourceLabels:
              - __meta_kubernetes_pod_node_name
            targetLabel: node
            replacement: ${1}
        # @schema
        # type: [null, array]
        # @schema
        # -- Metrics relabeling configs for the ServiceMonitor hubble
        metricRelabelings: ~
        # Configure TLS for the ServiceMonitor.
        # Note, when using TLS you will either need to specify
        # tlsConfig.insecureSkipVerify or specify a CA to use.
        tlsConfig: {}
      # -- Grafana dashboards for hubble
      # grafana can import dashboards based on the label and value
      # ref: https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-dashboards
      dashboards:
        enabled: false
        label: grafana_dashboard
        # @schema
        # type: [null, string]
        # @schema
        namespace: ~
        labelValue: "1"
        annotations: {}
      # Dynamic metrics  may be reconfigured without a need of agent restarts.
      dynamic:
        enabled: false
        config:
          # ---- Name of configmap with configuration that may be altered to reconfigure metric handlers within a running agent.
          configMapName: cilium-dynamic-metrics-config
          # ---- True if helm installer should create config map.
          # Switch to false if you want to self maintain the file content.
          createConfigMap: true
          # ---- Exporters configuration in YAML format.
          content:
            - name: all
              contextOptions: []
              includeFilters: []
              excludeFilters: []
    # -- Unix domain socket path to listen to when Hubble is enabled.
    socketPath: /var/run/cilium/hubble.sock
    # -- Enables redacting sensitive information present in Layer 7 flows.
    redact:
      enabled: false
      http:
        # -- Enables redacting URL query (GET) parameters.
        # Example:
        #
        #   redact:
        #     enabled: true
        #     http:
        #       urlQuery: true
        #
        # You can specify the options from the helm CLI:
        #
        #   --set hubble.redact.enabled="true"
        #   --set hubble.redact.http.urlQuery="true"
        urlQuery: false
        # -- Enables redacting user info, e.g., password when basic auth is used.
        # Example:
        #
        #   redact:
        #     enabled: true
        #     http:
        #       userInfo: true
        #
        # You can specify the options from the helm CLI:
        #
        #   --set hubble.redact.enabled="true"
        #   --set hubble.redact.http.userInfo="true"
        userInfo: true
        headers:
          # -- List of HTTP headers to allow: headers not matching will be redacted. Note: `allow` and `deny` lists cannot be used both at the same time, only one can be present.
          # Example:
          #   redact:
          #     enabled: true
          #     http:
          #       headers:
          #         allow:
          #           - traceparent
          #           - tracestate
          #           - Cache-Control
          #
          # You can specify the options from the helm CLI:
          #   --set hubble.redact.enabled="true"
          #   --set hubble.redact.http.headers.allow="traceparent,tracestate,Cache-Control"
          allow: []
          # -- List of HTTP headers to deny: matching headers will be redacted. Note: `allow` and `deny` lists cannot be used both at the same time, only one can be present.
          # Example:
          #   redact:
          #     enabled: true
          #     http:
          #       headers:
          #         deny:
          #           - Authorization
          #           - Proxy-Authorization
          #
          # You can specify the options from the helm CLI:
          #   --set hubble.redact.enabled="true"
          #   --set hubble.redact.http.headers.deny="Authorization,Proxy-Authorization"
          deny: []
      kafka:
        # -- Enables redacting Kafka's API key.
        # Example:
        #
        #   redact:
        #     enabled: true
        #     kafka:
        #       apiKey: true
        #
        # You can specify the options from the helm CLI:
        #
        #   --set hubble.redact.enabled="true"
        #   --set hubble.redact.kafka.apiKey="true"
        apiKey: false
    # -- An additional address for Hubble to listen to.
    # Set this field ":4244" if you are enabling Hubble Relay, as it assumes that
    # Hubble is listening on port 4244.
    listenAddress: ":4244"
    # -- Whether Hubble should prefer to announce IPv6 or IPv4 addresses if both are available.
    preferIpv6: false
    # @schema
    # type: [null, boolean]
    # @schema
    # -- (bool) Skip Hubble events with unknown cgroup ids
    # @default -- `true`
    skipUnknownCGroupIDs: ~
    peerService:
      # -- Service Port for the Peer service.
      # If not set, it is dynamically assigned to port 443 if TLS is enabled and to
      # port 80 if not.
      # servicePort: 80
      # -- Target Port for the Peer service, must match the hubble.listenAddress'
      # port.
      targetPort: 4244
      # -- The cluster domain to use to query the Hubble Peer service. It should
      # be the local cluster.
      clusterDomain: cluster.local
    # -- TLS configuration for Hubble
    tls:
      # -- Enable mutual TLS for listenAddress. Setting this value to false is
      # highly discouraged as the Hubble API provides access to potentially
      # sensitive network flow metadata and is exposed on the host network.
      enabled: false
      # -- Configure automatic TLS certificates generation.
      auto:
        # -- Auto-generate certificates.
        # When set to true, automatically generate a CA and certificates to
        # enable mTLS between Hubble server and Hubble Relay instances. If set to
        # false, the certs for Hubble server need to be provided by setting
        # appropriate values below.
        enabled: false

      # -- The Hubble server certificate and private key
      server:
        existingSecret: hubble-certs

    relay:
      # -- Enable Hubble Relay (requires hubble.enabled=true)
      enabled: true

    ui:
      # -- Whether to enable the Hubble UI.
      enabled: true

      # -- Roll out Hubble-ui pods automatically when configmap is updated.
      rollOutPods: true

      # -- The number of replicas of Hubble UI to deploy.
      replicas: 1


    # -- Emit v1.Events related to pods on detection of packet drops.
    #    This feature is alpha, please provide feedback at https://github.com/cilium/cilium/issues/33975.
    dropEventEmitter:
      enabled: false
      # --- Minimum time between emitting same events.
      interval: 2m
      # --- Drop reasons to emit events for.
      # ref: https://docs.cilium.io/en/stable/_api/v1/flow/README/#dropreason
      reasons:
        - auth_required
        - policy_denied

  # -- Method to use for identity allocation (`crd`, `kvstore` or `doublewrite-readkvstore` / `doublewrite-readcrd` for migrating between identity backends).
  identityAllocationMode: "crd"
  # -- (string) Time to wait before using new identity on endpoint identity change.
  # @default -- `"5s"`
  identityChangeGracePeriod: ""
  # -- Install Iptables rules to skip netfilter connection tracking on all pod
  # traffic. This option is only effective when Cilium is running in direct
  # routing and full KPR mode. Moreover, this option cannot be enabled when Cilium
  # is running in a managed Kubernetes environment or in a chained CNI setup.
  installNoConntrackIptablesRules: true
  ipam:
    # -- Configure IP Address Management mode.
    # ref: https://docs.cilium.io/en/stable/network/concepts/ipam/
    mode: "cluster-pool"
    # -- Maximum rate at which the CiliumNode custom resource is updated.
    ciliumNodeUpdateRate: "15s"
    # -- Pre-allocation settings for IPAM in Multi-Pool mode
    multiPoolPreAllocation: ""
    # -- Install ingress/egress routes through uplink on host for Pods when working with delegated IPAM plugin.
    installUplinkRoutesForDelegatedIPAM: false
    operator:
      # @schema
      # type: [array, string]
      # @schema
      # -- IPv4 CIDR list range to delegate to individual nodes for IPAM.
      clusterPoolIPv4PodCIDRList: ["172.20.52.0/22"]
      # -- IPv4 CIDR mask size to delegate to individual nodes for IPAM.
      clusterPoolIPv4MaskSize: 24
      # @schema
      # type: [array, string]
      # @schema
      # -- IPv6 CIDR list range to delegate to individual nodes for IPAM.
      clusterPoolIPv6PodCIDRList: ["fd00::/104"]
      # -- IPv6 CIDR mask size to delegate to individual nodes for IPAM.
      clusterPoolIPv6MaskSize: 120
      # -- IP pools to auto-create in multi-pool IPAM mode.
      autoCreateCiliumPodIPPools: {}
      #   default:
      #     ipv4:
      #       cidrs:
      #         - 10.10.0.0/8
      #       maskSize: 24
      #   other:
      #     ipv6:
      #       cidrs:
      #         - fd00:100::/80
      #       maskSize: 96
      # @schema
      # type: [null, integer]
      # @schema
      # -- (int) The maximum burst size when rate limiting access to external APIs.
      # Also known as the token bucket capacity.
      # @default -- `20`
      externalAPILimitBurstSize: ~
      # @schema
      # type: [null, number]
      # @schema
      # -- (float) The maximum queries per second when rate limiting access to
      # external APIs. Also known as the bucket refill rate, which is used to
      # refill the bucket up to the burst size capacity.
      # @default -- `4.0`
      externalAPILimitQPS: ~
  # -- defaultLBServiceIPAM indicates the default LoadBalancer Service IPAM when
  # no LoadBalancer class is set. Applicable values: lbipam, nodeipam, none
  # @schema
  # type: [string]
  # @schema
  defaultLBServiceIPAM: lbipam
  nodeIPAM:
    # -- Configure Node IPAM
    # ref: https://docs.cilium.io/en/stable/network/node-ipam/
    enabled: false
  # @schema
  # type: [null, string]
  # @schema
  # -- The api-rate-limit option can be used to overwrite individual settings of the default configuration for rate limiting calls to the Cilium Agent API
  apiRateLimit: ~
  # -- Configure the eBPF-based ip-masq-agent
  ipMasqAgent:
    enabled: false
  # the config of nonMasqueradeCIDRs
  # config:
  #   nonMasqueradeCIDRs: []
  #   masqLinkLocal: false
  #   masqLinkLocalIPv6: false

  # iptablesLockTimeout defines the iptables "--wait" option when invoked from Cilium.
  # iptablesLockTimeout: "5s"
  ipv4:
    # -- Enable IPv4 support.
    enabled: true
  ipv6:
    # -- Enable IPv6 support.
    enabled: false
  # -- Configure Kubernetes specific configuration
  k8s:
    # -- requireIPv4PodCIDR enables waiting for Kubernetes to provide the PodCIDR
    # range via the Kubernetes node resource
    requireIPv4PodCIDR: false
    # -- requireIPv6PodCIDR enables waiting for Kubernetes to provide the PodCIDR
    # range via the Kubernetes node resource
    requireIPv6PodCIDR: false
  # -- Keep the deprecated selector labels when deploying Cilium DaemonSet.
  keepDeprecatedLabels: false
  # -- Keep the deprecated probes when deploying Cilium DaemonSet
  keepDeprecatedProbes: false
  startupProbe:
    # -- failure threshold of startup probe.
    # 105 x 2s translates to the old behaviour of the readiness probe (120s delay + 30 x 3s)
    failureThreshold: 250
    # -- interval between checks of the startup probe
    periodSeconds: 5

  livenessProbe:
    # -- failure threshold of liveness probe
    failureThreshold: 10
    # -- interval between checks of the liveness probe
    periodSeconds: 30
  readinessProbe:
    # -- failure threshold of readiness probe
    failureThreshold: 3
    # -- interval between checks of the readiness probe
    periodSeconds: 30
  # -- Configure the kube-proxy replacement in Cilium BPF datapath
  # Valid options are "true" or "false".
  # ref: https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/
  kubeProxyReplacement: "true"

  # -- healthz server bind address for the kube-proxy replacement.
  # To enable set the value to '0.0.0.0:10256' for all ipv4
  # addresses and this '[::]:10256' for all ipv6 addresses.
  # By default it is disabled.
  kubeProxyReplacementHealthzBindAddr: ""
  l2NeighDiscovery:
    # -- Enable L2 neighbor discovery in the agent
    enabled: false
    # -- Override the agent's default neighbor resolution refresh period.
    refreshPeriod: "30s"

  # -- Enable Layer 7 network policy.
  l7Proxy: false

  # -- Enable Local Redirect Policy.
  localRedirectPolicy: false
  # To include or exclude matched resources from cilium identity evaluation
  # labels: ""

  # logOptions allows you to define logging options. eg:
  logOptions:
    format: json

  # -- Enables periodic logging of system load
  logSystemLoad: false
  # -- Configure maglev consistent hashing
  maglev: {}
  # -- tableSize is the size (parameter M) for the backend table of one
  # service entry
  # tableSize:

  # -- hashSeed is the cluster-wide base64 encoded seed for the hashing
  # hashSeed:

  # -- Enables masquerading of IPv4 traffic leaving the node from endpoints.
  enableIPv4Masquerade: true
  # -- Enables masquerading of IPv6 traffic leaving the node from endpoints.
  enableIPv6Masquerade: true
  # -- Enables masquerading to the source of the route for traffic leaving the node from endpoints.
  enableMasqueradeRouteSource: false
  # -- Enables IPv4 BIG TCP support which increases maximum IPv4 GSO/GRO limits for nodes and pods
  enableIPv4BIGTCP: false
  # -- Enables IPv6 BIG TCP support which increases maximum IPv6 GSO/GRO limits for nodes and pods
  enableIPv6BIGTCP: false
  nat:
    # -- Number of the top-k SNAT map connections to track in Cilium statedb.
    mapStatsEntries: 32
    # -- Interval between how often SNAT map is counted for stats.
    mapStatsInterval: 30s

  egressGateway:
    # -- Enables egress gateway to redirect and SNAT the traffic that leaves the
    # cluster.
    enabled: true
    # -- Time between triggers of egress gateway state reconciliations
    reconciliationTriggerInterval: 1s
    # -- Maximum number of entries in egress gateway policy map
    # maxPolicyEntries: 16384

  vtep:
    # -- Enables VXLAN Tunnel Endpoint (VTEP) Integration (beta) to allow
    # Cilium-managed pods to talk to third party VTEP devices over Cilium tunnel.
    enabled: false

  # -- (string) Allows to explicitly specify the IPv4 CIDR for native routing.
  # When specified, Cilium assumes networking for this CIDR is preconfigured and
  # hands traffic destined for that range to the Linux network stack without
  # applying any SNAT.
  # Generally speaking, specifying a native routing CIDR implies that Cilium can
  # depend on the underlying networking stack to route packets to their
  # destination. To offer a concrete example, if Cilium is configured to use
  # direct routing and the Kubernetes CIDR is included in the native routing CIDR,
  # the user must configure the routes to reach pods, either manually or by
  # setting the auto-direct-node-routes flag.
  ipv4NativeRoutingCIDR: "0.0.0.0/0"
  # -- (string) Allows to explicitly specify the IPv6 CIDR for native routing.
  # When specified, Cilium assumes networking for this CIDR is preconfigured and
  # hands traffic destined for that range to the Linux network stack without
  # applying any SNAT.
  # Generally speaking, specifying a native routing CIDR implies that Cilium can
  # depend on the underlying networking stack to route packets to their
  # destination. To offer a concrete example, if Cilium is configured to use
  # direct routing and the Kubernetes CIDR is included in the native routing CIDR,
  # the user must configure the routes to reach pods, either manually or by
  # setting the auto-direct-node-routes flag.
  ipv6NativeRoutingCIDR: ""

  # -- cilium-monitor sidecar.
  monitor:
    # -- Enable the cilium-monitor sidecar.
    enabled: false

  # -- Configure service load balancing
  loadBalancer:
    # -- standalone enables the standalone L4LB which does not connect to
    # kube-apiserver.
    # standalone: false

    # -- algorithm is the name of the load balancing algorithm for backend
    # selection e.g. random or maglev
    algorithm: maglev

    # -- mode is the operation mode of load balancing for remote backends
    # e.g. snat, dsr, hybrid
    mode: snat

    # -- acceleration is the option to accelerate service handling via XDP
    # Applicable values can be: disabled (do not use XDP), native (XDP BPF
    # program is run directly out of the networking driver's early receive
    # path), or best-effort (use native mode XDP acceleration on devices
    # that support it).
    acceleration: disabled
    # -- dsrDispatch configures whether IP option or IPIP encapsulation is
    # used to pass a service IP and port to remote backend
    dsrDispatch: ''

    # -- serviceTopology enables K8s Topology Aware Hints -based service
    # endpoints filtering
    # serviceTopology: false

    # -- experimental enables support for the experimental load-balancing
    # control-plane.
    experimental: false
    # -- L7 LoadBalancer
    l7:
      # -- Enable L7 service load balancing via envoy proxy.
      # The request to a k8s service, which has specific annotation e.g. service.cilium.io/lb-l7,
      # will be forwarded to the local backend proxy to be load balanced to the service endpoints.
      # Please refer to docs for supported annotations for more configuration.
      #
      # Applicable values:
      #   - envoy: Enable L7 load balancing via envoy proxy. This will automatically set enable-envoy-config as well.
      #   - disabled: Disable L7 load balancing by way of service annotation.
      backend: disabled
      # -- List of ports from service to be automatically redirected to above backend.
      # Any service exposing one of these ports will be automatically redirected.
      # Fine-grained control can be achieved by using the service annotation.
      ports: []
      # -- Default LB algorithm
      # The default LB algorithm to be used for services, which can be overridden by the
      # service annotation (e.g. service.cilium.io/lb-l7-algorithm)
      # Applicable values: round_robin, least_request, random
      algorithm: round_robin

  # -- Configure N-S k8s service loadbalancing
  nodePort:
    # -- Enable the Cilium NodePort service implementation.
    enabled: false
    # -- Port range to use for NodePort services.
    # range: "30000,32767"

    # @schema
    # type: [null, string, array]
    # @schema
    # -- List of CIDRs for choosing which IP addresses assigned to native devices are used for NodePort load-balancing.
    # By default this is empty and the first suitable, preferably private, IPv4 and IPv6 address assigned to each device is used.
    #
    # Example:
    #
    #   addresses: ["192.168.1.0/24", "2001::/64"]
    #
    addresses:
      - 172.31.241.238/30
    # -- Set to true to prevent applications binding to service ports.
    bindProtection: true
    # -- Append NodePort range to ip_local_reserved_ports if clash with ephemeral
    # ports is detected.
    autoProtectPortRange: true
    # -- Enable healthcheck nodePort server for NodePort services
    enableHealthCheck: true
    # -- Enable access of the healthcheck nodePort on the LoadBalancerIP. Needs
    # EnableHealthCheck to be enabled
    enableHealthCheckLoadBalancerIP: false
  # policyAuditMode: false

  # -- The agent can be put into one of the three policy enforcement modes:
  # default, always and never.
  # ref: https://docs.cilium.io/en/stable/security/policy/intro/#policy-enforcement-modes
  policyEnforcementMode: "default"
  # @schema
  # type: [null, string, array]
  # @schema
  # -- policyCIDRMatchMode is a list of entities that may be selected by CIDR selector.
  # The possible value is "nodes".
  policyCIDRMatchMode:
  pprof:
    # -- Enable pprof for cilium-agent
    enabled: false
    # -- Configure pprof listen address for cilium-agent
    address: localhost
    # -- Configure pprof listen port for cilium-agent
    port: 6060
  # -- Configure prometheus metrics on the configured port at /metrics
  prometheus:
    metricsService: true
    enabled: true
    port: 9962
    serviceMonitor:
      # -- Enable service monitors.
      # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
      enabled: true

      # -- Set to `true` and helm will not check for monitoring.coreos.com/v1 CRDs before deploying
      trustCRDsExist: true

      relabelings:
        - action: replace

          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: node
          replacement: ${1}

    # @schema
    # type: [null, array]
    # @schema
    # -- Metrics that should be enabled or disabled from the default metric list.
    # The list is expected to be separated by a space. (+metric_foo to enable
    # metric_foo , -metric_bar to disable metric_bar).
    # ref: https://docs.cilium.io/en/stable/observability/metrics/
    metrics:
      - +cilium_bpf_map_pressure

    # --- Enable controller group metrics for monitoring specific Cilium
    # subsystems. The list is a list of controller group names. The special
    # values of "all" and "none" are supported. The set of controller
    # group names is not guaranteed to be stable between Cilium versions.
    controllerGroupMetrics:
      - write-cni-file
      - sync-host-ips
      - sync-lb-maps-with-k8s-services
  # -- Grafana dashboards for cilium-agent
  # grafana can import dashboards based on the label and value
  # ref: https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-dashboards
  dashboards:
    enabled: true

  # Configure Cilium Envoy options.
  envoy:
    # @schema
    # type: [null, boolean]
    # @schema
    # -- Enable Envoy Proxy in standalone DaemonSet.
    # This field is enabled by default for new installation.
    # @default -- `true` for new installation
    enabled: ~
    # -- (int)
    # Set Envoy'--base-id' to use when allocating shared memory regions.
    # Only needs to be changed if multiple Envoy instances will run on the same node and may have conflicts. Supported values: 0 - 4294967295. Defaults to '0'
    baseID: 0
    log:
      # @schema
      # type: [null, string]
      # @schema
      # -- The format string to use for laying out the log message metadata of Envoy. If specified, Envoy will use text format output.
      # This setting is mutually exclusive with envoy.log.format_json.
      format: "[%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v"
      # @schema
      # type: [null, object]
      # @schema
      # -- The JSON logging format to use for Envoy. This setting is mutually exclusive with envoy.log.format.
      # ref: https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/bootstrap/v3/bootstrap.proto#envoy-v3-api-field-config-bootstrap-v3-bootstrap-applicationlogconfig-logformat-json-format
      format_json: null
      # date: "%Y-%m-%dT%T.%e"
      # thread_id: "%t"
      # source_line: "%s:%#"
      # level: "%l"
      # logger: "%n"
      # message: "%j"
      # -- Path to a separate Envoy log file, if any. Defaults to /dev/stdout.
      path: ""
      # @schema
      # oneOf:
      # - type: [null]
      # - enum: [trace,debug,info,warning,error,critical,off]
      # @schema
      # -- Default log level of Envoy application log that is configured if Cilium debug / verbose logging isn't enabled.
      # This option allows to have a different log level than the Cilium Agent - e.g. lower it to `critical`.
      # Possible values: trace, debug, info, warning, error, critical, off
      # @default -- Defaults to the default log level of the Cilium Agent - `info`
      defaultLevel: ~
    # -- Time in seconds after which a TCP connection attempt times out
    connectTimeoutSeconds: 2
    # -- Time in seconds after which the initial fetch on an xDS stream is considered timed out
    initialFetchTimeoutSeconds: 30
    # -- Maximum number of concurrent retries on Envoy clusters
    maxConcurrentRetries: 128
    # -- Maximum number of retries for each HTTP request
    httpRetryCount: 3
    # -- ProxyMaxRequestsPerConnection specifies the max_requests_per_connection setting for Envoy
    maxRequestsPerConnection: 0
    # -- Set Envoy HTTP option max_connection_duration seconds. Default 0 (disable)
    maxConnectionDurationSeconds: 0
    # -- Set Envoy upstream HTTP idle connection timeout seconds.
    # Does not apply to connections with pending requests. Default 60s
    idleTimeoutDurationSeconds: 60
    # -- Number of trusted hops regarding the x-forwarded-for and related HTTP headers for the ingress L7 policy enforcement Envoy listeners.
    xffNumTrustedHopsL7PolicyIngress: 0
    # -- Number of trusted hops regarding the x-forwarded-for and related HTTP headers for the egress L7 policy enforcement Envoy listeners.
    xffNumTrustedHopsL7PolicyEgress: 0
    # -- Envoy container image.
    image:
      # @schema
      # type: [null, string]
      # @schema
      override: ~
      repository: "quay.io/cilium/cilium-envoy"
      tag: "v1.31.5-1736159621-690fc2d580e2590559756d81ed59561c75c950aa"
      pullPolicy: "IfNotPresent"
      digest: "sha256:701db990a3e8c82ac8002e6c7a5526de80f871a7388bcb33d51c33897d153f39"
      useDigest: true
    # -- Additional containers added to the cilium Envoy DaemonSet.
    extraContainers: []
    # -- Additional envoy container arguments.
    extraArgs: []
    # -- Additional envoy container environment variables.
    extraEnv: []
    # -- Additional envoy hostPath mounts.
    extraHostPathMounts: []
    # - name: host-mnt-data
    #   mountPath: /host/mnt/data
    #   hostPath: /mnt/data
    #   hostPathType: Directory
    #   readOnly: true
    #   mountPropagation: HostToContainer

    # -- Additional envoy volumes.
    extraVolumes: []
    # -- Additional envoy volumeMounts.
    extraVolumeMounts: []
    # -- Configure termination grace period for cilium-envoy DaemonSet.
    terminationGracePeriodSeconds: 1
    # -- TCP port for the health API.
    healthPort: 9878
    # -- cilium-envoy update strategy
    # ref: https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#updating-a-daemonset
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        # @schema
        # type: [integer, string]
        # @schema
        maxUnavailable: 2
    # -- Roll out cilium envoy pods automatically when configmap is updated.
    rollOutPods: false
    # -- ADVANCED OPTION: Bring your own custom Envoy bootstrap ConfigMap. Provide the name of a ConfigMap with a `bootstrap-config.json` key.
    # When specified, Envoy will use this ConfigMap instead of the default provided by the chart.
    # WARNING: Use of this setting has the potential to prevent cilium-envoy from starting up, and can cause unexpected behavior (e.g. due to
    # syntax error or semantically incorrect configuration). Before submitting an issue, please ensure you have disabled this feature, as support
    # cannot be provided for custom Envoy bootstrap configs.
    # @schema
    # type: [null, string]
    # @schema
    bootstrapConfigMap: ~
    # -- Annotations to be added to all top-level cilium-envoy objects (resources under templates/cilium-envoy)
    annotations: {}
    # -- Security Context for cilium-envoy pods.
    podSecurityContext:
      # -- AppArmorProfile options for the `cilium-agent` and init containers
      appArmorProfile:
        type: "Unconfined"
    # -- Annotations to be added to envoy pods
    podAnnotations: {}
    # -- Labels to be added to envoy pods
    podLabels: {}
    # -- Envoy resource limits & requests
    # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    resources: {}
    #   limits:
    #     cpu: 4000m
    #     memory: 4Gi
    #   requests:
    #     cpu: 100m
    #     memory: 512Mi

    startupProbe:
      # -- failure threshold of startup probe.
      # 105 x 2s translates to the old behaviour of the readiness probe (120s delay + 30 x 3s)
      failureThreshold: 105
      # -- interval between checks of the startup probe
      periodSeconds: 2
    livenessProbe:
      # -- failure threshold of liveness probe
      failureThreshold: 10
      # -- interval between checks of the liveness probe
      periodSeconds: 30
    readinessProbe:
      # -- failure threshold of readiness probe
      failureThreshold: 3
      # -- interval between checks of the readiness probe
      periodSeconds: 30
    securityContext:
      # -- User to run the pod with
      # runAsUser: 0
      # -- Run the pod with elevated privileges
      privileged: false
      # -- SELinux options for the `cilium-envoy` container
      seLinuxOptions:
        level: 's0'
        # Running with spc_t since we have removed the privileged mode.
        # Users can change it to a different type as long as they have the
        # type available on the system.
        type: 'spc_t'
      capabilities:
        # -- Capabilities for the `cilium-envoy` container.
        # Even though granted to the container, the cilium-envoy-starter wrapper drops
        # all capabilities after forking the actual Envoy process.
        # `NET_BIND_SERVICE` is the only capability that can be passed to the Envoy process by
        # setting `envoy.securityContext.capabilities.keepNetBindService=true` (in addition to granting the
        # capability to the container).
        # Note: In case of embedded envoy, the capability must  be granted to the cilium-agent container.
        envoy:
          # Used since cilium proxy uses setting IPPROTO_IP/IP_TRANSPARENT
          - NET_ADMIN
          # We need it for now but might not need it for >= 5.11 specially
          # for the 'SYS_RESOURCE'.
          # In >= 5.8 there's already BPF and PERMON capabilities
          - SYS_ADMIN
          # Both PERFMON and BPF requires kernel 5.8, container runtime
          # cri-o >= v1.22.0 or containerd >= v1.5.0.
          # If available, SYS_ADMIN can be removed.
          #- PERFMON
          #- BPF
        # -- Keep capability `NET_BIND_SERVICE` for Envoy process.
        keepCapNetBindService: false
    # -- Affinity for cilium-envoy.
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                k8s-app: cilium-envoy
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                k8s-app: cilium
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: cilium.io/no-schedule
                  operator: NotIn
                  values:
                    - "true"
    # -- Node selector for cilium-envoy.
    nodeSelector:
      kubernetes.io/os: linux
    # -- Node tolerations for envoy scheduling to nodes with taints
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    tolerations:
      - operator: Exists
        # - key: "key"
        #   operator: "Equal|Exists"
        #   value: "value"
        #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"
    # @schema
    # type: [null, string]
    # @schema
    # -- The priority class to use for cilium-envoy.
    priorityClassName: ~
    # @schema
    # type: [null, string]
    # @schema
    # -- DNS policy for Cilium envoy pods.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
    dnsPolicy: ~
    debug:
      admin:
        # -- Enable admin interface for cilium-envoy.
        # This is useful for debugging and should not be enabled in production.
        enabled: false
        # -- Port number (bound to loopback interface).
        # kubectl port-forward can be used to access the admin interface.
        port: 9901
    # -- Configure Cilium Envoy Prometheus options.
    # Note that some of these apply to either cilium-agent or cilium-envoy.
    prometheus:
      # -- Enable prometheus metrics for cilium-envoy
      enabled: true
      serviceMonitor:
        # -- Enable service monitors.
        # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
        # Note that this setting applies to both cilium-envoy _and_ cilium-agent
        # with Envoy enabled.
        enabled: false
        # -- Labels to add to ServiceMonitor cilium-envoy
        labels: {}
        # -- Annotations to add to ServiceMonitor cilium-envoy
        annotations: {}
        # -- Interval for scrape metrics.
        interval: "10s"
        # -- Specify the Kubernetes namespace where Prometheus expects to find
        # service monitors configured.
        # namespace: ""
        # -- Relabeling configs for the ServiceMonitor cilium-envoy
        # or for cilium-agent with Envoy configured.
        relabelings:
          - sourceLabels:
              - __meta_kubernetes_pod_node_name
            targetLabel: node
            replacement: ${1}
        # @schema
        # type: [null, array]
        # @schema
        # -- Metrics relabeling configs for the ServiceMonitor cilium-envoy
        # or for cilium-agent with Envoy configured.
        metricRelabelings: ~
      # -- Serve prometheus metrics for cilium-envoy on the configured port
      port: "9964"
  # -- Enable/Disable use of node label based identity
  nodeSelectorLabels: false
  # -- Enable resource quotas for priority classes used in the cluster.
  resourceQuotas:
    enabled: false
    cilium:
      hard:
        # 5k nodes * 2 DaemonSets (Cilium and cilium node init)
        pods: "10k"
    operator:
      hard:
        # 15 "clusterwide" Cilium Operator pods for HA
        pods: "15"
  # Need to document default
  ##################
  #sessionAffinity: false

  # -- Do not run Cilium agent when running with clean mode. Useful to completely
  # uninstall Cilium as it will stop Cilium from starting and create artifacts
  # in the node.
  sleepAfterInit: false

  # -- Enable check of service source ranges (currently, only for LoadBalancer).
  svcSourceRangeCheck: true

  # -- Synchronize Kubernetes nodes to kvstore and perform CNP GC.
  synchronizeK8sNodes: true

  # -- Configure TLS configuration in the agent.
  tls:
    # -- This configures how the Cilium agent loads the secrets used TLS-aware CiliumNetworkPolicies
    # (namely the secrets referenced by terminatingTLS and originatingTLS).
    # Possible values:
    #   - local
    #   - k8s
    secretsBackend: local
    # -- Configures settings for synchronization of TLS Interception Secrets
    secretSync:
      # -- Enable synchronization of Secrets for TLS Interception. If disabled and
      # tls.secretsBackend is set to 'k8s', then secrets will be read directly by the agent.
      enabled: true
      # -- This configures secret synchronization for secrets used in CiliumNetworkPolicies
      secretsNamespace:
        # -- Create secrets namespace for TLS Interception secrets.
        create: true
        # -- Name of TLS Interception secret namespace.
        name: cilium-secrets
    # -- Base64 encoded PEM values for the CA certificate and private key.
    # This can be used as common CA to generate certificates used by hubble and clustermesh components.
    # It is neither required nor used when cert-manager is used to generate the certificates.
    ca:
      # -- Optional CA cert. If it is provided, it will be used by cilium to
      # generate all other certificates. Otherwise, an ephemeral CA is generated.
      cert: ""
      # -- Optional CA private key. If it is provided, it will be used by cilium to
      # generate all other certificates. Otherwise, an ephemeral CA is generated.
      key: ""
      # -- Generated certificates validity duration in days. This will be used for auto generated CA.
      certValidityDuration: 1095
    # -- Configure the CA trust bundle used for the validation of the certificates
    # leveraged by hubble and clustermesh. When enabled, it overrides the content of the
    # 'ca.crt' field of the respective certificates, allowing for CA rotation with no down-time.
    caBundle:
      # -- Enable the use of the CA trust bundle.
      enabled: false
      # -- Name of the ConfigMap containing the CA trust bundle.
      name: cilium-root-ca.crt
      # -- Entry of the ConfigMap containing the CA trust bundle.
      key: ca.crt
      # -- Use a Secret instead of a ConfigMap.
      useSecret: false
      # If uncommented, creates the ConfigMap and fills it with the specified content.
      # Otherwise, the ConfigMap is assumed to be already present in .Release.Namespace.
      #
      # content: |
      #   -----BEGIN CERTIFICATE-----
      #   ...
      #   -----END CERTIFICATE-----
      #   -----BEGIN CERTIFICATE-----
      #   ...
      #   -----END CERTIFICATE-----
  # -- Tunneling protocol to use in tunneling mode and for ad-hoc tunnels.
  # Possible values:
  #   - ""
  #   - vxlan
  #   - geneve
  # @default -- `"vxlan"`
  tunnelProtocol: ''

  # -- Enable native-routing mode or tunneling mode.
  # Possible values:
  #   - ""
  #   - native
  #   - tunnel
  # @default -- `"tunnel"`
  routingMode: native

  # -- Configure VXLAN and Geneve tunnel port.
  # @default -- Port 8472 for VXLAN, Port 6081 for Geneve
  tunnelPort: 0

  # -- Configure what the response should be to traffic for a service without backends.
  # Possible values:
  #  - reject (default)
  #  - drop
  serviceNoBackendResponse: reject
  
  # -- Configure the underlying network MTU to overwrite auto-detected MTU.
  # This value doesn't change the host network interface MTU i.e. eth0 or ens0.
  # It changes the MTU for cilium_net@cilium_host, cilium_host@cilium_net,
  # cilium_vxlan and lxc_health interfaces.
  MTU: 9000
  
  # -- Disable the usage of CiliumEndpoint CRD.
  disableEndpointCRD: false
  wellKnownIdentities:
    # -- Enable the use of well-known identities.
    enabled: false
  etcd:
    # -- Enable etcd mode for the agent.
    enabled: false

  operator:
    # -- Enable the cilium-operator component (required).
    enabled: true
    # -- Roll out cilium-operator pods automatically when configmap is updated.
    rollOutPods: true

    # -- Number of replicas to run for the cilium-operator deployment
    replicas: 1
    # -- The priority class to use for cilium-operator
    priorityClassName: system-cluster-critical
    # -- DNS policy for Cilium operator pods.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
    dnsPolicy: ""
    # -- cilium-operator update strategy
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        # @schema
        # type: [integer, string]
        # @schema
        maxSurge: 25%
        # @schema
        # type: [integer, string]
        # @schema
        maxUnavailable: 50%

    # -- Pod topology spread constraints for cilium-operator
    topologySpreadConstraints: []
    #   - maxSkew: 1
    #     topologyKey: topology.kubernetes.io/zone
    #     whenUnsatisfiable: DoNotSchedule

    # -- Node labels for cilium-operator pod assignment
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    nodeSelector:
      kubernetes.io/os: linux
    # -- Node tolerations for cilium-operator scheduling to nodes with taints
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    tolerations:
      - operator: Exists
        # - key: "key"
        #   operator: "Equal|Exists"
        #   value: "value"
        #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"
    # -- Additional cilium-operator container arguments.
    extraArgs: []
    # -- Additional cilium-operator environment variables.
    extraEnv: []
    # -- Additional cilium-operator hostPath mounts.
    extraHostPathMounts: []
    # - name: host-mnt-data
    #   mountPath: /host/mnt/data
    #   hostPath: /mnt/data
    #   hostPathType: Directory
    #   readOnly: true
    #   mountPropagation: HostToContainer

    # -- Additional cilium-operator volumes.
    extraVolumes: []
    # -- Additional cilium-operator volumeMounts.
    extraVolumeMounts: []
    # -- Annotations to be added to all top-level cilium-operator objects (resources under templates/cilium-operator)
    annotations: {}
    # -- HostNetwork setting
    hostNetwork: true
    # -- Security context to be added to cilium-operator pods
    podSecurityContext: {}
    # -- Annotations to be added to cilium-operator pods
    podAnnotations: {}
    # -- Labels to be added to cilium-operator pods
    podLabels: {}
    # PodDisruptionBudget settings
    podDisruptionBudget:
      # -- enable PodDisruptionBudget
      # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
      enabled: false
      # @schema
      # type: [null, integer, string]
      # @schema
      # -- Minimum number/percentage of pods that should remain scheduled.
      # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
      minAvailable: null
      # @schema
      # type: [null, integer, string]
      # @schema
      # -- Maximum number/percentage of pods that may be made unavailable
      maxUnavailable: 1

    # -- Interval for endpoint garbage collection.
    endpointGCInterval: "5m0s"
    # -- Interval for cilium node garbage collection.
    nodeGCInterval: "5m0s"
    # -- Interval for identity garbage collection.
    identityGCInterval: "15m0s"
    # -- Timeout for identity heartbeats.
    identityHeartbeatTimeout: "30m0s"

    pprof:
      # -- Enable pprof for cilium-operator
      enabled: false
      # -- Configure pprof listen address for cilium-operator
      address: localhost
      # -- Configure pprof listen port for cilium-operator
      port: 6061

    # -- Enable prometheus metrics for cilium-operator on the configured port at
    # /metrics
    prometheus:
      metricsService: false
      enabled: true
      port: 9963
      serviceMonitor:
        # -- Enable service monitors.
        # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
        enabled: true

    # -- Grafana dashboards for cilium-operator
    # grafana can import dashboards based on the label and value
    # ref: https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-dashboards
    dashboards:
      enabled: false
      label: grafana_dashboard
      # @schema
      # type: [null, string]
      # @schema
      namespace: ~
      labelValue: "1"
      annotations: {}
    # -- Skip CRDs creation for cilium-operator
    skipCRDCreation: false
    # -- Remove Cilium node taint from Kubernetes nodes that have a healthy Cilium
    # pod running.
    removeNodeTaints: true
    # @schema
    # type: [null, boolean]
    # @schema
    # -- Taint nodes where Cilium is scheduled but not running. This prevents pods
    # from being scheduled to nodes where Cilium is not the default CNI provider.
    # @default -- same as removeNodeTaints
    setNodeTaints: ~
    # -- Set Node condition NetworkUnavailable to 'false' with the reason
    # 'CiliumIsUp' for nodes that have a healthy Cilium pod.
    setNodeNetworkStatus: true
    unmanagedPodWatcher:
      # -- Restart any pod that are not managed by Cilium.
      restart: true
      # -- Interval, in seconds, to check if there are any pods that are not
      # managed by Cilium.
      intervalSeconds: 15
  nodeinit:
    # -- Enable the node initialization DaemonSet
    enabled: false
    # -- node-init image.
    image:
      # @schema
      # type: [null, string]
      # @schema
      override: ~
      repository: "quay.io/cilium/startup-script"
      tag: "c54c7edeab7fde4da68e59acd319ab24af242c3f"
      digest: "sha256:8d7b41c4ca45860254b3c19e20210462ef89479bb6331d6760c4e609d651b29c"
      useDigest: true
      pullPolicy: "IfNotPresent"
    # -- The priority class to use for the nodeinit pod.
    priorityClassName: ""
    # -- node-init update strategy
    updateStrategy:
      type: RollingUpdate
    # -- Additional nodeinit environment variables.
    extraEnv: []
    # -- Additional nodeinit volumes.
    extraVolumes: []
    # -- Additional nodeinit volumeMounts.
    extraVolumeMounts: []
    # -- Affinity for cilium-nodeinit
    affinity: {}
    # -- Node labels for nodeinit pod assignment
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    nodeSelector:
      kubernetes.io/os: linux
    # -- Node tolerations for nodeinit scheduling to nodes with taints
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    tolerations:
      - operator: Exists
        # - key: "key"
        #   operator: "Equal|Exists"
        #   value: "value"
        #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"
    # -- Annotations to be added to all top-level nodeinit objects (resources under templates/cilium-nodeinit)
    annotations: {}
    # -- Annotations to be added to node-init pods.
    podAnnotations: {}
    # -- Labels to be added to node-init pods.
    podLabels: {}
    # -- Security Context for cilium-node-init pods.
    podSecurityContext:
      # -- AppArmorProfile options for the `cilium-node-init` and init containers
      appArmorProfile:
        type: "Unconfined"
    # -- nodeinit resource limits & requests
    # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    # -- Security context to be added to nodeinit pods.
    securityContext:
      privileged: false
      seLinuxOptions:
        level: 's0'
        # Running with spc_t since we have removed the privileged mode.
        # Users can change it to a different type as long as they have the
        # type available on the system.
        type: 'spc_t'
      capabilities:
        add:
          # Used in iptables. Consider removing once we are iptables-free
          - SYS_MODULE
          # Used for nsenter
          - NET_ADMIN
          - SYS_ADMIN
          - SYS_CHROOT
          - SYS_PTRACE
    # -- bootstrapFile is the location of the file where the bootstrap timestamp is
    # written by the node-init DaemonSet
    bootstrapFile: "/tmp/cilium-bootstrap.d/cilium-bootstrap-time"
    # -- startup offers way to customize startup nodeinit script (pre and post position)
    startup:
      preScript: ""
      postScript: ""
    # -- prestop offers way to customize prestop nodeinit script (pre and post position)
    prestop:
      preScript: ""
      postScript: ""
  preflight:
    # -- Enable Cilium pre-flight resources (required for upgrade)
    enabled: false
    # -- Cilium pre-flight image.
    image:
      # @schema
      # type: [null, string]
      # @schema
      override: ~
      repository: "quay.io/cilium/cilium"
      tag: "v1.17.0-rc.1"
      # cilium-digest
      digest: "sha256:a6e0b8285dc7979e89d2cec34fbd0c5997a459f1870ebd42c8ece3e061cf5f7e"
      useDigest: true
      pullPolicy: "IfNotPresent"
    # -- The priority class to use for the preflight pod.
    priorityClassName: ""
    # -- preflight update strategy
    updateStrategy:
      type: RollingUpdate
    # -- Additional preflight environment variables.
    extraEnv: []
    # -- Additional preflight volumes.
    extraVolumes: []
    # -- Additional preflight volumeMounts.
    extraVolumeMounts: []
    # -- Affinity for cilium-preflight
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                k8s-app: cilium
    # -- Node labels for preflight pod assignment
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    nodeSelector:
      kubernetes.io/os: linux
    # -- Node tolerations for preflight scheduling to nodes with taints
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    tolerations:
      - operator: Exists
        # - key: "key"
        #   operator: "Equal|Exists"
        #   value: "value"
        #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"
    # -- Annotations to be added to all top-level preflight objects (resources under templates/cilium-preflight)
    annotations: {}
    # -- Security context to be added to preflight pods.
    podSecurityContext: {}
    # -- Annotations to be added to preflight pods
    podAnnotations: {}
    # -- Labels to be added to the preflight pod.
    podLabels: {}
    # PodDisruptionBudget settings
    podDisruptionBudget:
      # -- enable PodDisruptionBudget
      # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
      enabled: false
      # @schema
      # type: [null, integer, string]
      # @schema
      # -- Minimum number/percentage of pods that should remain scheduled.
      # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
      minAvailable: null
      # @schema
      # type: [null, integer, string]
      # @schema
      # -- Maximum number/percentage of pods that may be made unavailable
      maxUnavailable: 1
    # -- preflight resource limits & requests
    # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    resources: {}
    #   limits:
    #     cpu: 4000m
    #     memory: 4Gi
    #   requests:
    #     cpu: 100m
    #     memory: 512Mi

    readinessProbe:
      # -- For how long kubelet should wait before performing the first probe
      initialDelaySeconds: 5
      # -- interval between checks of the readiness probe
      periodSeconds: 5
    # -- Security context to be added to preflight pods
    securityContext: {}
    #   runAsUser: 0

    # -- Path to write the `--tofqdns-pre-cache` file to.
    tofqdnsPreCache: ""
    # -- Configure termination grace period for preflight Deployment and DaemonSet.
    terminationGracePeriodSeconds: 1
    # -- By default we should always validate the installed CNPs before upgrading
    # Cilium. This will make sure the user will have the policies deployed in the
    # cluster with the right schema.
    validateCNPs: true
  # -- Explicitly enable or disable priority class.
  # .Capabilities.KubeVersion is unsettable in `helm template` calls,
  # it depends on k8s libraries version that Helm was compiled against.
  # This option allows to explicitly disable setting the priority class, which
  # is useful for rendering charts for gke clusters in advance.
  enableCriticalPriorityClass: true
  # disableEnvoyVersionCheck removes the check for Envoy, which can be useful
  # on AArch64 as the images do not currently ship a version of Envoy.
  #disableEnvoyVersionCheck: false
  clustermesh:
    # -- Deploy clustermesh-apiserver for clustermesh
    useAPIServer: false
    # -- The maximum number of clusters to support in a ClusterMesh. This value
    # cannot be changed on running clusters, and all clusters in a ClusterMesh
    # must be configured with the same value. Values > 255 will decrease the
    # maximum allocatable cluster-local identities.
    # Supported values are 255 and 511.
    maxConnectedClusters: 255
    # -- Enable the synchronization of Kubernetes EndpointSlices corresponding to
    # the remote endpoints of appropriately-annotated global services through ClusterMesh
    enableEndpointSliceSynchronization: false
    # -- Enable Multi-Cluster Services API support
    enableMCSAPISupport: false
    # -- Annotations to be added to all top-level clustermesh objects (resources under templates/clustermesh-apiserver and templates/clustermesh-config)
    annotations: {}
    # -- Clustermesh explicit configuration.
    config:
      # -- Enable the Clustermesh explicit configuration.
      enabled: false
      # -- Default dns domain for the Clustermesh API servers
      # This is used in the case cluster addresses are not provided
      # and IPs are used.
      domain: mesh.cilium.io
      # -- List of clusters to be peered in the mesh.
      clusters: []
      # clusters:
      # # -- Name of the cluster
      # - name: cluster1
      # # -- Address of the cluster, use this if you created DNS records for
      # # the cluster Clustermesh API server.
      #   address: cluster1.mesh.cilium.io
      # # -- Port of the cluster Clustermesh API server.
      #   port: 2379
      # # -- IPs of the cluster Clustermesh API server, use multiple ones when
      # # you have multiple IPs to access the Clustermesh API server.
      #   ips:
      #   - 172.18.255.201
      # # -- base64 encoded PEM values for the cluster client certificate, private key and certificate authority.
      # # These fields can (and should) be omitted in case the CA is shared across clusters. In that case, the
      # # "remote" private key and certificate available in the local cluster are automatically used instead.
      #   tls:
      #     cert: ""
      #     key: ""
      #     caCert: ""
    apiserver:

      # -- TCP port for the clustermesh-apiserver health API.
      healthPort: 9880
      # -- Configuration for the clustermesh-apiserver readiness probe.
      readinessProbe: {}
      etcd:
        # The etcd binary is included in the clustermesh API server image, so the same image from above is reused.
        # Independent override isn't supported, because clustermesh-apiserver is tested against the etcd version it is
        # built with.

        # -- Specifies the resources for etcd container in the apiserver
        resources: {}
        #   requests:
        #     cpu: 200m
        #     memory: 256Mi
        #   limits:
        #     cpu: 1000m
        #     memory: 256Mi

        # -- Security context to be added to clustermesh-apiserver etcd containers
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
        # -- lifecycle setting for the etcd container
        lifecycle: {}
        init:
          # -- Specifies the resources for etcd init container in the apiserver
          resources: {}
          #   requests:
          #     cpu: 100m
          #     memory: 100Mi
          #   limits:
          #     cpu: 100m
          #     memory: 100Mi

          # -- Additional arguments to `clustermesh-apiserver etcdinit`.
          extraArgs: []
          # -- Additional environment variables to `clustermesh-apiserver etcdinit`.
          extraEnv: []
        # @schema
        # enum: [Disk, Memory]
        # @schema
        # -- Specifies whether etcd data is stored in a temporary volume backed by
        # the node's default medium, such as disk, SSD or network storage (Disk), or
        # RAM (Memory). The Memory option enables improved etcd read and write
        # performance at the cost of additional memory usage, which counts against
        # the memory limits of the container.
        storageMedium: Disk
      kvstoremesh:
        # -- Enable KVStoreMesh. KVStoreMesh caches the information retrieved
        # from the remote clusters in the local etcd instance.
        enabled: true
        # -- TCP port for the KVStoreMesh health API.
        healthPort: 9881
        # -- Configuration for the KVStoreMesh readiness probe.
        readinessProbe: {}
        # -- Additional KVStoreMesh arguments.
        extraArgs: []
        # -- Additional KVStoreMesh environment variables.
        extraEnv: []
        # -- Resource requests and limits for the KVStoreMesh container
        resources: {}
        #   requests:
        #     cpu: 100m
        #     memory: 64Mi
        #   limits:
        #     cpu: 1000m
        #     memory: 1024M

        # -- Additional KVStoreMesh volumeMounts.
        extraVolumeMounts: []
        # -- KVStoreMesh Security context
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
        # -- lifecycle setting for the KVStoreMesh container
        lifecycle: {}
      service:
        # -- The type of service used for apiserver access.
        type: NodePort
        # -- Optional port to use as the node port for apiserver access.
        #
        # WARNING: make sure to configure a different NodePort in each cluster if
        # kube-proxy replacement is enabled, as Cilium is currently affected by a known
        # bug (#24692) when NodePorts are handled by the KPR implementation. If a service
        # with the same NodePort exists both in the local and the remote cluster, all
        # traffic originating from inside the cluster and targeting the corresponding
        # NodePort will be redirected to a local backend, regardless of whether the
        # destination node belongs to the local or the remote cluster.
        nodePort: 32379
        # -- Annotations for the clustermesh-apiserver service.
        # Example annotations to configure an internal load balancer on different cloud providers:
        # * AKS: service.beta.kubernetes.io/azure-load-balancer-internal: "true"
        # * EKS: service.beta.kubernetes.io/aws-load-balancer-scheme: "internal"
        # * GKE: networking.gke.io/load-balancer-type: "Internal"
        annotations: {}
        # @schema
        # enum: [Local, Cluster]
        # @schema
        # -- The externalTrafficPolicy of service used for apiserver access.
        externalTrafficPolicy: Cluster
        # @schema
        # enum: [Local, Cluster]
        # @schema
        # -- The internalTrafficPolicy of service used for apiserver access.
        internalTrafficPolicy: Cluster
        # @schema
        # enum: [HAOnly, Always, Never]
        # @schema
        # -- Defines when to enable session affinity.
        # Each replica in a clustermesh-apiserver deployment runs its own discrete
        # etcd cluster. Remote clients connect to one of the replicas through a
        # shared Kubernetes Service. A client reconnecting to a different backend
        # will require a full resync to ensure data integrity. Session affinity
        # can reduce the likelihood of this happening, but may not be supported
        # by all cloud providers.
        # Possible values:
        #  - "HAOnly" (default) Only enable session affinity for deployments with more than 1 replica.
        #  - "Always" Always enable session affinity.
        #  - "Never" Never enable session affinity. Useful in environments where
        #            session affinity is not supported, but may lead to slightly
        #            degraded performance due to more frequent reconnections.
        enableSessionAffinity: "HAOnly"
        # @schema
        # type: [null, string]
        # @schema
        # -- Configure a loadBalancerClass.
        # Allows to configure the loadBalancerClass on the clustermesh-apiserver
        # LB service in case the Service type is set to LoadBalancer
        # (requires Kubernetes 1.24+).
        loadBalancerClass: ~
        # @schema
        # type: [null, string]
        # @schema
        # -- Configure a specific loadBalancerIP.
        # Allows to configure a specific loadBalancerIP on the clustermesh-apiserver
        # LB service in case the Service type is set to LoadBalancer.
        loadBalancerIP: ~
        # -- Configure loadBalancerSourceRanges.
        # Allows to configure the source IP ranges allowed to access the
        # clustermesh-apiserver LB service in case the Service type is set to LoadBalancer.
        loadBalancerSourceRanges: []
      # -- Number of replicas run for the clustermesh-apiserver deployment.
      replicas: 1
      # -- lifecycle setting for the apiserver container
      lifecycle: {}
      # -- terminationGracePeriodSeconds for the clustermesh-apiserver deployment
      terminationGracePeriodSeconds: 30
      # -- Additional clustermesh-apiserver arguments.
      extraArgs: []
      # -- Additional clustermesh-apiserver environment variables.
      extraEnv: []
      # -- Additional clustermesh-apiserver volumes.
      extraVolumes: []
      # -- Additional clustermesh-apiserver volumeMounts.
      extraVolumeMounts: []
      # -- Security context to be added to clustermesh-apiserver containers
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
      # -- Security context to be added to clustermesh-apiserver pods
      podSecurityContext:
        runAsNonRoot: true
        runAsUser: 65532
        runAsGroup: 65532
        fsGroup: 65532
      # -- Annotations to be added to clustermesh-apiserver pods
      podAnnotations: {}
      # -- Labels to be added to clustermesh-apiserver pods
      podLabels: {}
      # PodDisruptionBudget settings
      podDisruptionBudget:
        # -- enable PodDisruptionBudget
        # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
        enabled: false
        # @schema
        # type: [null, integer, string]
        # @schema
        # -- Minimum number/percentage of pods that should remain scheduled.
        # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
        minAvailable: null
        # @schema
        # type: [null, integer, string]
        # @schema
        # -- Maximum number/percentage of pods that may be made unavailable
        maxUnavailable: 1
      # -- Resource requests and limits for the clustermesh-apiserver
      resources: {}
      #   requests:
      #     cpu: 100m
      #     memory: 64Mi
      #   limits:
      #     cpu: 1000m
      #     memory: 1024M

      # -- Affinity for clustermesh.apiserver
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    k8s-app: clustermesh-apiserver
                topologyKey: kubernetes.io/hostname
      # -- Pod topology spread constraints for clustermesh-apiserver
      topologySpreadConstraints: []
      #   - maxSkew: 1
      #     topologyKey: topology.kubernetes.io/zone
      #     whenUnsatisfiable: DoNotSchedule

      # -- Node labels for pod assignment
      # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
      nodeSelector:
        kubernetes.io/os: linux
      # -- Node tolerations for pod assignment on nodes with taints
      # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
      tolerations: []
      # -- clustermesh-apiserver update strategy
      updateStrategy:
        type: RollingUpdate
        rollingUpdate:
          # @schema
          # type: [integer, string]
          # @schema
          maxSurge: 1
          # @schema
          # type: [integer, string]
          # @schema
          maxUnavailable: 0
      # -- The priority class to use for clustermesh-apiserver
      priorityClassName: ""
      tls:
        # -- Configure the clustermesh authentication mode.
        # Supported values:
        # - legacy:     All clusters access remote clustermesh instances with the same
        #               username (i.e., remote). The "remote" certificate must be
        #               generated with CN=remote if provided manually.
        # - migration:  Intermediate mode required to upgrade from legacy to cluster
        #               (and vice versa) with no disruption. Specifically, it enables
        #               the creation of the per-cluster usernames, while still using
        #               the common one for authentication. The "remote" certificate must
        #               be generated with CN=remote if provided manually (same as legacy).
        # - cluster:    Each cluster accesses remote etcd instances with a username
        #               depending on the local cluster name (i.e., remote-<cluster-name>).
        #               The "remote" certificate must be generated with CN=remote-<cluster-name>
        #               if provided manually. Cluster mode is meaningful only when the same
        #               CA is shared across all clusters part of the mesh.
        authMode: legacy
        # -- Allow users to provide their own certificates
        # Users may need to provide their certificates using
        # a mechanism that requires they provide their own secrets.
        # This setting does not apply to any of the auto-generated
        # mechanisms below, it only restricts the creation of secrets
        # via the `tls-provided` templates.
        enableSecrets: true
        # -- Configure automatic TLS certificates generation.
        # A Kubernetes CronJob is used the generate any
        # certificates not provided by the user at installation
        # time.
        auto:
          # -- When set to true, automatically generate a CA and certificates to
          # enable mTLS between clustermesh-apiserver and external workload instances.
          # If set to false, the certs to be provided by setting appropriate values below.
          enabled: true
          # Sets the method to auto-generate certificates. Supported values:
          # - helm:         This method uses Helm to generate all certificates.
          # - cronJob:      This method uses a Kubernetes CronJob the generate any
          #                 certificates not provided by the user at installation
          #                 time.
          # - certmanager:  This method use cert-manager to generate & rotate certificates.
          method: helm
          # -- Generated certificates validity duration in days.
          certValidityDuration: 1095
          # -- Schedule for certificates regeneration (regardless of their expiration date).
          # Only used if method is "cronJob". If nil, then no recurring job will be created.
          # Instead, only the one-shot job is deployed to generate the certificates at
          # installation time.
          #
          # Due to the out-of-band distribution of client certs to external workloads the
          # CA is (re)regenerated only if it is not provided as a helm value and the k8s
          # secret is manually deleted.
          #
          # Defaults to none. Commented syntax gives midnight of the first day of every
          # fourth month. For syntax, see
          # https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#schedule-syntax
          # schedule: "0 0 1 */4 *"

          # [Example]
          # certManagerIssuerRef:
          #   group: cert-manager.io
          #   kind: ClusterIssuer
          #   name: ca-issuer
          # -- certmanager issuer used when clustermesh.apiserver.tls.auto.method=certmanager.
          certManagerIssuerRef: {}
        # -- base64 encoded PEM values for the clustermesh-apiserver server certificate and private key.
        # Used if 'auto' is not enabled.
        server:
          cert: ""
          key: ""
          # -- Extra DNS names added to certificate when it's auto generated
          extraDnsNames: []
          # -- Extra IP addresses added to certificate when it's auto generated
          extraIpAddresses: []
        # -- base64 encoded PEM values for the clustermesh-apiserver admin certificate and private key.
        # Used if 'auto' is not enabled.
        admin:
          cert: ""
          key: ""
        # -- base64 encoded PEM values for the clustermesh-apiserver client certificate and private key.
        # Used if 'auto' is not enabled.
        client:
          cert: ""
          key: ""
        # -- base64 encoded PEM values for the clustermesh-apiserver remote cluster certificate and private key.
        # Used if 'auto' is not enabled.
        remote:
          cert: ""
          key: ""
      # clustermesh-apiserver Prometheus metrics configuration
      metrics:
        # -- Enables exporting apiserver metrics in OpenMetrics format.
        enabled: true
        # -- Configure the port the apiserver metric server listens on.
        port: 9962
        kvstoremesh:
          # -- Enables exporting KVStoreMesh metrics in OpenMetrics format.
          enabled: true
          # -- Configure the port the KVStoreMesh metric server listens on.
          port: 9964
        etcd:
          # -- Enables exporting etcd metrics in OpenMetrics format.
          enabled: true
          # -- Set level of detail for etcd metrics; specify 'extensive' to include server side gRPC histogram metrics.
          mode: basic
          # -- Configure the port the etcd metric server listens on.
          port: 9963
        serviceMonitor:
          # -- Enable service monitor.
          # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
          enabled: false
          # -- Labels to add to ServiceMonitor clustermesh-apiserver
          labels: {}
          # -- Annotations to add to ServiceMonitor clustermesh-apiserver
          annotations: {}
          # -- Specify the Kubernetes namespace where Prometheus expects to find
          # service monitors configured.
          # namespace: ""

          # -- Interval for scrape metrics (apiserver metrics)
          interval: "10s"
          # @schema
          # type: [null, array]
          # @schema
          # -- Relabeling configs for the ServiceMonitor clustermesh-apiserver (apiserver metrics)
          relabelings: ~
          # @schema
          # type: [null, array]
          # @schema
          # -- Metrics relabeling configs for the ServiceMonitor clustermesh-apiserver (apiserver metrics)
          metricRelabelings: ~
          kvstoremesh:
            # -- Interval for scrape metrics (KVStoreMesh metrics)
            interval: "10s"
            # @schema
            # type: [null, array]
            # @schema
            # -- Relabeling configs for the ServiceMonitor clustermesh-apiserver (KVStoreMesh metrics)
            relabelings: ~
            # @schema
            # type: [null, array]
            # @schema
            # -- Metrics relabeling configs for the ServiceMonitor clustermesh-apiserver (KVStoreMesh metrics)
            metricRelabelings: ~
          etcd:
            # -- Interval for scrape metrics (etcd metrics)
            interval: "10s"
            # @schema
            # type: [null, array]
            # @schema
            # -- Relabeling configs for the ServiceMonitor clustermesh-apiserver (etcd metrics)
            relabelings: ~
            # @schema
            # type: [null, array]
            # @schema
            # -- Metrics relabeling configs for the ServiceMonitor clustermesh-apiserver (etcd metrics)
            metricRelabelings: ~


  # -- Configure external workloads support
  externalWorkloads:
    # -- Enable support for external workloads, such as VMs (false by default).
    enabled: false

  # -- Configure cgroup related configuration
  cgroup:
    autoMount:
      # -- Enable auto mount of cgroup2 filesystem.
      # When `autoMount` is enabled, cgroup2 filesystem is mounted at
      # `cgroup.hostRoot` path on the underlying host and inside the cilium agent pod.
      # If users disable `autoMount`, it's expected that users have mounted
      # cgroup2 filesystem at the specified `cgroup.hostRoot` volume, and then the
      # volume will be mounted inside the cilium agent pod at the same path.
      enabled: true
      # -- Init Container Cgroup Automount resource limits & requests
      resources: {}
      #   limits:
      #     cpu: 100m
      #     memory: 128Mi
      #   requests:
      #     cpu: 100m
      #     memory: 128Mi
    # -- Configure cgroup root where cgroup2 filesystem is mounted on the host (see also: `cgroup.autoMount`)
    hostRoot: /run/cilium/cgroupv2

  # -- Configure sysctl override described in #20072.
  sysctlfix:
    # -- Enable the sysctl override. When enabled, the init container will mount the /proc of the host so that the `sysctlfix` utility can execute.
    enabled: true

  # -- Configure whether to enable auto detect of terminating state for endpoints
  # in order to support graceful termination.
  enableK8sTerminatingEndpoint: true

  # -- Configure whether to unload DNS policy rules on graceful shutdown
  # dnsPolicyUnloadOnShutdown: false

  # -- Configure the key of the taint indicating that Cilium is not ready on the node.
  # When set to a value starting with `ignore-taint.cluster-autoscaler.kubernetes.io/`, the Cluster Autoscaler will ignore the taint on its decisions, allowing the cluster to scale up.
  agentNotReadyTaintKey: "node.cilium.io/agent-not-ready"

  dnsProxy:
    # -- Timeout (in seconds) when closing the connection between the DNS proxy and the upstream server. If set to 0, the connection is closed immediately (with TCP RST). If set to -1, the connection is closed asynchronously in the background.
    socketLingerTimeout: 10
    # -- DNS response code for rejecting DNS requests, available options are '[nameError refused]'.
    dnsRejectResponseCode: refused
    # -- Allow the DNS proxy to compress responses to endpoints that are larger than 512 Bytes or the EDNS0 option, if present.
    enableDnsCompression: true
    # -- Maximum number of IPs to maintain per FQDN name for each endpoint.
    endpointMaxIpPerHostname: 1000
    # -- Time during which idle but previously active connections with expired DNS lookups are still considered alive.
    idleConnectionGracePeriod: 0s
    # -- Maximum number of IPs to retain for expired DNS lookups with still-active connections.
    maxDeferredConnectionDeletes: 10000
    # -- The minimum time, in seconds, to use DNS data for toFQDNs policies. If
    # the upstream DNS server returns a DNS record with a shorter TTL, Cilium
    # overwrites the TTL with this value. Setting this value to zero means that
    # Cilium will honor the TTLs returned by the upstream DNS server.
    minTtl: 0
    # -- DNS cache data at this path is preloaded on agent startup.
    preCache: ""
    # -- Global port on which the in-agent DNS proxy should listen. Default 0 is a OS-assigned port.
    proxyPort: 0
    # -- The maximum time the DNS proxy holds an allowed DNS response before sending it along. Responses are sent as soon as the datapath is updated with the new IP information.
    proxyResponseMaxDelay: 100ms
    # -- DNS proxy operation mode (true/false, or unset to use version dependent defaults)
    # enableTransparentMode: true

  # -- SCTP Configuration Values
  sctp:
    # -- Enable SCTP support. NOTE: Currently, SCTP support does not support rewriting ports or multihoming.
    enabled: false

  # -- Enable Non-Default-Deny policies
  enableNonDefaultDenyPolicies: true

  # Configuration for types of authentication for Cilium (beta)
  authentication:
    # -- Enable authentication processing and garbage collection.
    # Note that if disabled, policy enforcement will still block requests that require authentication.
    # But the resulting authentication requests for these requests will not be processed, therefore the requests not be allowed.
    enabled: true
    # -- Buffer size of the channel Cilium uses to receive authentication events from the signal map.
    queueSize: 1024
    # -- Buffer size of the channel Cilium uses to receive certificate expiration events from auth handlers.
    rotatedIdentitiesQueueSize: 1024
    # -- Interval for garbage collection of auth map entries.
    gcInterval: "5m0s"
    # Configuration for Cilium's service-to-service mutual authentication using TLS handshakes.
    # Note that this is not full mTLS support without also enabling encryption of some form.
    # Current encryption options are WireGuard or IPsec, configured in encryption block above.
    mutual:
      # -- Port on the agent where mutual authentication handshakes between agents will be performed
      port: 4250
      # -- Timeout for connecting to the remote node TCP socket
      connectTimeout: 5s
      # Settings for SPIRE
      spire:
        # -- Enable SPIRE integration (beta)
        enabled: false
        # -- Annotations to be added to all top-level spire objects (resources under templates/spire)
        annotations: {}
        # Settings to control the SPIRE installation and configuration
        install:
          # -- Enable SPIRE installation.
          # This will only take effect only if authentication.mutual.spire.enabled is true
          enabled: true
          # -- SPIRE namespace to install into
          namespace: cilium-spire
          # -- SPIRE namespace already exists. Set to true if Helm should not create, manage, and import the SPIRE namespace.
          existingNamespace: false
          # -- init container image of SPIRE agent and server
          initImage:
            # @schema
            # type: [null, string]
            # @schema
            override: ~
            repository: "docker.io/library/busybox"
            tag: "1.37.0"
            digest: "sha256:2919d0172f7524b2d8df9e50066a682669e6d170ac0f6a49676d54358fe970b5"
            useDigest: true
            pullPolicy: "IfNotPresent"
          # SPIRE agent configuration
          agent:
            # -- The priority class to use for the spire agent
            priorityClassName: ""
            # -- SPIRE agent image
            image:
              # @schema
              # type: [null, string]
              # @schema
              override: ~
              repository: "ghcr.io/spiffe/spire-agent"
              tag: "1.9.6"
              digest: "sha256:5106ac601272a88684db14daf7f54b9a45f31f77bb16a906bd5e87756ee7b97c"
              useDigest: true
              pullPolicy: "IfNotPresent"
            # -- SPIRE agent service account
            serviceAccount:
              create: true
              name: spire-agent
            # -- SPIRE agent annotations
            annotations: {}
            # -- SPIRE agent labels
            labels: {}
            # -- container resource limits & requests
            resources: {}
            # -- SPIRE Workload Attestor kubelet verification.
            skipKubeletVerification: true
            # -- SPIRE agent tolerations configuration
            # By default it follows the same tolerations as the agent itself
            # to allow the Cilium agent on this node to connect to SPIRE.
            # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
            tolerations:
              - key: node.kubernetes.io/not-ready
                effect: NoSchedule
              - key: node-role.kubernetes.io/master
                effect: NoSchedule
              - key: node-role.kubernetes.io/control-plane
                effect: NoSchedule
              - key: node.cloudprovider.kubernetes.io/uninitialized
                effect: NoSchedule
                value: "true"
              - key: CriticalAddonsOnly
                operator: "Exists"
            # -- SPIRE agent affinity configuration
            affinity: {}
            # -- SPIRE agent nodeSelector configuration
            # ref: ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
            nodeSelector: {}
            # -- Security context to be added to spire agent pods.
            # SecurityContext holds pod-level security attributes and common container settings.
            # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
            podSecurityContext: {}
            # -- Security context to be added to spire agent containers.
            # SecurityContext holds pod-level security attributes and common container settings.
            # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
            securityContext: {}
          server:
            # -- The priority class to use for the spire server
            priorityClassName: 'system-cluster-critical'
            # -- SPIRE server image
            image:
              # @schema
              # type: [null, string]
              # @schema
              override: ~
              repository: "ghcr.io/spiffe/spire-server"
              tag: "1.9.6"
              digest: "sha256:59a0b92b39773515e25e68a46c40d3b931b9c1860bc445a79ceb45a805cab8b4"
              useDigest: true
              pullPolicy: "IfNotPresent"
            # -- SPIRE server service account
            serviceAccount:
              create: true
              name: spire-server
            # -- SPIRE server init containers
            initContainers: []
            # -- SPIRE server annotations
            annotations: {}
            # -- SPIRE server labels
            labels: {}
            # SPIRE server service configuration
            # -- container resource limits & requests
            resources: {}
            service:
              # -- Service type for the SPIRE server service
              type: ClusterIP
              # -- Annotations to be added to the SPIRE server service
              annotations: {}
              # -- Labels to be added to the SPIRE server service
              labels: {}
            # -- SPIRE server affinity configuration
            affinity: {}
            # -- SPIRE server nodeSelector configuration
            # ref: ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
            nodeSelector: {}
            # -- SPIRE server tolerations configuration
            # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
            tolerations: []
            # SPIRE server datastorage configuration
            dataStorage:
              # -- Enable SPIRE server data storage
              enabled: true
              # -- Size of the SPIRE server data storage
              size: 1Gi
              # -- Access mode of the SPIRE server data storage
              accessMode: ReadWriteOnce
              # @schema
              # type: [null, string]
              # @schema
              # -- StorageClass of the SPIRE server data storage
              storageClass: null
            # -- Security context to be added to spire server pods.
            # SecurityContext holds pod-level security attributes and common container settings.
            # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
            podSecurityContext: {}
            # -- Security context to be added to spire server containers.
            # SecurityContext holds pod-level security attributes and common container settings.
            # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
            securityContext: {}
            # SPIRE CA configuration
            ca:
              # -- SPIRE CA key type
              # AWS requires the use of RSA. EC cryptography is not supported
              keyType: "rsa-4096"
              # -- SPIRE CA Subject
              subject:
                country: "US"
                organization: "SPIRE"
                commonName: "Cilium SPIRE CA"
        # @schema
        # type: [null, string]
        # @schema
        # -- SPIRE server address used by Cilium Operator
        #
        # If k8s Service DNS along with port number is used (e.g. <service-name>.<namespace>.svc(.*):<port-number> format),
        # Cilium Operator will resolve its address by looking up the clusterIP from Service resource.
        #
        # Example values: 10.0.0.1:8081, spire-server.cilium-spire.svc:8081
        serverAddress: ~
        # -- SPIFFE trust domain to use for fetching certificates
        trustDomain: spiffe.cilium
        # -- SPIRE socket path where the SPIRE delegated api agent is listening
        adminSocketPath: /run/spire/sockets/admin.sock
        # -- SPIRE socket path where the SPIRE workload agent is listening.
        # Applies to both the Cilium Agent and Operator
        agentSocketPath: /run/spire/sockets/agent/agent.sock
        # -- SPIRE connection timeout
        connectionTimeout: 30s
  # -- Enable Internal Traffic Policy
  enableInternalTrafficPolicy: true
  # -- Enable LoadBalancer IP Address Management
  enableLBIPAM: true

external-dns:

  enabled: true
  ## @param fullnameOverride String to fully override external-dns.fullname template
  ##
  fullnameOverride: ext-dns

  ## @param clusterDomain Kubernetes Cluster Domain
  ##
  clusterDomain: cluster.local

  ## @param commonLabels Labels to add to all deployed objects
  ##
  commonLabels: {}
  ## @param commonAnnotations Annotations to add to all deployed objects
  ##
  commonAnnotations:
    argocd.argoproj.io/sync-options: ServerSideApply=false
  ##
  ## @param extraDeploy Array of extra objects to deploy with the release (evaluated as a template).
  ##
  extraDeploy: []
  ## @param kubeVersion Force target Kubernetes version (using Helm capabilities if not set)
  ##
  kubeVersion: ""

  ## @param watchReleaseNamespace Watch only namepsace used for the release
  ##
  watchReleaseNamespace: false

  ## @section external-dns parameters
  ##

  ## Bitnami external-dns image version
  ## ref: https://hub.docker.com/r/bitnami/external-dns/tags/
  ## @param image.registry ExternalDNS image registry
  ## @param image.repository ExternalDNS image repository
  ## @param image.tag ExternalDNS Image tag (immutable tags are recommended)
  ## @param image.digest ExternalDNS image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param image.pullPolicy ExternalDNS image pull policy
  ## @param image.pullSecrets ExternalDNS image pull secrets
  ##
  image:
    registry: docker.io
    repository: bitnami/external-dns

    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##
    pullPolicy: IfNotPresent

    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []


  ## @param updateStrategy update strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#update-strategies
  ##
  updateStrategy: {}

  ## @param command Override kiam default command
  ##
  command: []

  ## @param args Override kiam default args
  ##
  args: []

  ## @param sources [array] K8s resources type to be observed for new DNS entries by ExternalDNS
  ##
  sources:
    - crd
    - service
    - ingress
    - gateway-httproute
    - gateway-grpcroute
    - gateway-tlsroute
    - gateway-tcproute
    - gateway-udproute


  ## @param provider DNS provider where the DNS records will be created.
  ## Available providers are:
  ## - alibabacloud, aws, azure, azure-private-dns, cloudflare, coredns, designate, digitalocean, google, hetzner, infoblox, linode, rfc2136, transip, oci
  ##
  provider: cloudflare

  ## @param fqdnTemplates Templated strings that are used to generate DNS names from sources that don't define a hostname themselves
  ##
  fqdnTemplates: []

  ## @param combineFQDNAnnotation Combine FQDN template and annotations instead of overwriting
  ##
  combineFQDNAnnotation: false

  ## @param ignoreHostnameAnnotation Ignore hostname annotation when generating DNS names, valid only when fqdn-template is set
  ##
  ignoreHostnameAnnotation: false

  ## @param publishInternalServices Allow external-dns to publish DNS records for ClusterIP services
  ##
  publishInternalServices: true

  ## @param publishHostIP Allow external-dns to publish host-ip for headless services
  ##
  publishHostIP: false

  ## @param serviceTypeFilter The service types to take care about (default: all, options: ClusterIP, NodePort, LoadBalancer, ExternalName)
  ##
  serviceTypeFilter: []


  ## Cloudflare configuration to be set via arguments/env. variables
  ##
  cloudflare:
    ## @param cloudflare.secretName When using the Cloudflare provider, it's the name of the secret containing cloudflare_api_token or cloudflare_api_key.
    ## This ignores cloudflare.apiToken, and cloudflare.apiKey
    ##
    secretName: kjdev-cloudflare

    proxied: false

  ## @param domainFilters Limit possible target zones by domain suffixes (optional)
  ##
  domainFilters: []

  ## @param excludeDomains Exclude subdomains (optional)
  ##
  excludeDomains: []

  ## @param regexDomainFilter Limit possible target zones by regex domain suffixes (optional)
  ## If regexDomainFilter is specified, domainFilters will be ignored
  ##
  regexDomainFilter: ""

  ## @param regexDomainExclusion Exclude subdomains by using regex pattern (optional)
  ## If regexDomainFilter is specified, excludeDomains will be ignored and external-dns will use regexDomainExclusion even though regexDomainExclusion is empty
  ##
  regexDomainExclusion: ""

  ## @param zoneNameFilters Filter target zones by zone domain (optional)
  ##
  zoneNameFilters: []

  ## @param zoneIdFilters Limit possible target zones by zone id (optional)
  ##
  zoneIdFilters: []

  ## @param annotationFilter Filter sources managed by external-dns via annotation using label selector (optional)
  ##
  annotationFilter: ""

  ## @param labelFilter Select sources managed by external-dns using label selector (optional)
  ##
  labelFilter: 'wan-mode=public'

  ## @param dryRun When enabled, prints DNS record changes rather than actually performing them (optional)
  ##
  dryRun: false

  ## @param triggerLoopOnEvent When enabled, triggers run loop on create/update/delete events in addition to regular interval (optional)
  ##
  triggerLoopOnEvent: false

  ## @param interval Interval update period to use
  ##
  interval: '1m'

  ## @param logLevel Verbosity of the logs (options: panic, debug, info, warning, error, fatal, trace)
  ##
  logLevel: trace

  ## @param logFormat Which format to output logs in (options: text, json)
  ##
  logFormat: json

  ## @param policy Modify how DNS records are synchronized between sources and providers (options: sync, upsert-only )
  ##
  policy: sync

  ## @param registry Registry method to use (options: txt, aws-sd, noop)
  ## ref: https://github.com/kubernetes-sigs/external-dns/blob/master/docs/proposal/registry.md
  ##
  registry: "txt"

  ## @param txtPrefix When using the TXT registry, a prefix for ownership records that avoids collision with CNAME entries (optional)<CNAME record> (Mutual exclusive with txt-suffix)
  ##
  txtPrefix: ""

  ## @param txtSuffix When using the TXT registry, a suffix for ownership records that avoids collision with CNAME entries (optional)<CNAME record>.suffix (Mutual exclusive with txt-prefix)
  ##
  txtSuffix: ""

  ## @param txtOwnerId A name that identifies this instance of ExternalDNS. Currently used by registry types: txt & aws-sd (optional)
  ## But other registry types might be added in the future.
  ##
  txtOwnerId: k0s-dc1

  ## @param forceTxtOwnerId (backward compatibility) When using the non-TXT registry, it will pass the value defined by `txtOwnerId` down to the application (optional)
  ## This setting added for backward compatibility for
  ## customers who already used bitnami/external-dns helm chart
  ## to privision 'aws-sd' registry type.
  ## Previously bitnami/external-dns helm chart did not pass
  ## txtOwnerId value down to the external-dns application
  ## so the app itself sets that value to be a string 'default'.
  ## If existing customers force the actual txtOwnerId value to be
  ## passed properly, their external-dns updates will stop working
  ## because the owner's value for exting DNS records in
  ## AWS Service Discovery would remain 'default'.
  ## NOTE: It is up to the end user to update AWS Service Discovery
  ## 'default' values in description fields to make it work with new
  ## value passed as txtOwnerId when forceTxtOwnerId=true
  ##
  forceTxtOwnerId: false

  ## @param extraArgs Extra arguments to be passed to external-dns
  ##
  extraArgs: {}
  ## @param extraEnvVars An array to add extra env vars
  ##
  extraEnvVars: []
  ## @param extraEnvVarsCM ConfigMap containing extra env vars
  ##
  extraEnvVarsCM: ""
  ## @param extraEnvVarsSecret Secret containing extra env vars (in case of sensitive data)
  ##
  extraEnvVarsSecret: ""
  ## @param lifecycleHooks [object] Override default etcd container hooks
  ##
  lifecycleHooks: {}
  ## @param schedulerName Alternative scheduler
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param topologySpreadConstraints Topology Spread Constraints for pod assignment
  ## https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  ## The value is evaluated as a template
  ##
  topologySpreadConstraints: []
  ## @param replicaCount Desired number of ExternalDNS replicas
  ##
  replicaCount: 1
  ## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ## Allowed values: soft, hard
  ##
  podAntiAffinityPreset: soft
  ## Node affinity preset
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param nodeAffinityPreset.key Node label key to match Ignored if `affinity` is set.
    ## E.g.
    ## key: "kubernetes.io/e2e-az-name"
    ##
    key: ""
    ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set.
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param affinity Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## @param tolerations Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param podAnnotations Additional annotations to apply to the pod.
  ##
  podAnnotations: {}
  ## @param podLabels Additional labels to be added to pods
  ##
  podLabels: {}
  ## @param priorityClassName priorityClassName
  ##
  priorityClassName: 'system-cluster-critical'
  ## @param secretAnnotations Additional annotations to apply to the secret
  ##
  secretAnnotations: {}
  ## Options for the source type "crd"
  ##
  crd:
    ## @param crd.create Install and use the integrated DNSEndpoint CRD
    ##
    create: true
    ## @param crd.apiversion Sets the API version for the CRD to watch
    ##
    apiversion: externaldns.k8s.io/v1alpha1
    ## @param crd.kind Sets the kind for the CRD to watch
    ##
    kind: DNSEndpoint

  ## Kubernetes svc configutarion
  ##
  service:
    ## @param service.enabled Whether to create Service resource or not
    ##
    enabled: true
    ## @param service.type Kubernetes Service type
    ##
    type: ClusterIP
    ## @param service.ports.http ExternalDNS client port
    ##
    ports:
      http: 7979
    ## @param service.nodePorts.http Port to bind to for NodePort service type (client port)
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
    ##
    nodePorts:
      http: ""
    ## @param service.clusterIP IP address to assign to service
    ##
    clusterIP: ""
    ## @param service.externalIPs Service external IP addresses
    ##
    externalIPs: []
    ## @param service.loadBalancerIP IP address to assign to load balancer (if supported)
    ##
    loadBalancerIP: ""
    ## @param service.loadBalancerSourceRanges List of IP CIDRs allowed access to load balancer (if supported)
    ##
    loadBalancerSourceRanges: []
    ## @param service.externalTrafficPolicy Enable client source IP preservation
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param service.extraPorts Extra ports to expose in the service (normally used with the `sidecar` value)
    ##
    extraPorts: []
    ## @param service.annotations Annotations to add to service
    ## set the LoadBalancer service type to internal only.
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
    ##
    annotations: {}
    ## @param service.labels Provide any additional labels which may be required.
    ## This can be used to have external-dns show up in `kubectl cluster-info`
    ##  kubernetes.io/cluster-service: "true"
    ##  kubernetes.io/name: "external-dns"
    ##
    labels: {}
    ## @param service.sessionAffinity Session Affinity for Kubernetes service, can be "None" or "ClientIP"
    ## If "ClientIP", consecutive client requests will be directed to the same Pod
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies
    ##
    sessionAffinity: None
    ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}

  ## RBAC parameters
  ## https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  ##
  rbac:
    ## @param rbac.create Whether to create & use RBAC resources or not
    ##
    create: true

    ## @param rbac.clusterRole Whether to create Cluster Role. When set to false creates a Role in `namespace`
    ##
    clusterRole: true

    ## @param rbac.apiVersion Version of the RBAC API
    ##
    apiVersion: v1

    ## @param rbac.pspEnabled Whether to create a PodSecurityPolicy. WARNING: PodSecurityPolicy is deprecated in Kubernetes v1.21 or later, unavailable in v1.25 or later
    ##
    pspEnabled: false







  ## Configure extra options for startup probe
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-startup-probes/#configure-probes
  ## @param startupProbe.enabled Enable startupProbe
  ## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param startupProbe.periodSeconds Period seconds for startupProbe
  ## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1


  ## @param customLivenessProbe Override default liveness probe
  ##
  customLivenessProbe: {}

  ## @param customReadinessProbe Override default readiness probe
  ##
  customReadinessProbe: {}

  ## @param customStartupProbe Override default startup probe
  ##
  customStartupProbe: {}

  ## @param extraVolumes A list of volumes to be added to the pod
  ##
  extraVolumes: []

  ## @param extraVolumeMounts A list of volume mounts to be added to the pod
  ##
  extraVolumeMounts: []

  ## @param podDisruptionBudget Configure PodDisruptionBudget
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  ##

  podDisruptionBudget: {}

  ## Prometheus Exporter / Metrics
  ##
  metrics:
    ## @param metrics.enabled Enable prometheus to access external-dns metrics endpoint
    ##
    enabled: false

    ## @param metrics.podAnnotations Annotations for enabling prometheus to access the metrics endpoint
    ##
    podAnnotations: {}

    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor object
      ##
      enabled: false

      ## @param metrics.serviceMonitor.namespace Namespace in which Prometheus is running
      ##
      namespace: ""

      ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ""

      ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ""

      ## @param metrics.serviceMonitor.selector Additional labels for ServiceMonitor object
      ## ref: https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator#prometheus-configuration
      ## e.g:
      ## selector:
      ##   prometheus: my-prometheus
      ##
      selector: {}

      ## @param metrics.serviceMonitor.metricRelabelings Specify Metric Relabelings to add to the scrape endpoint
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []

      ## @param metrics.serviceMonitor.relabelings [array] Prometheus relabeling rules
      ##
      relabelings: []

      ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false

      ## DEPRECATED metrics.serviceMonitor.additionalLabels will be removed in a future release - Please use metrics.serviceMonitor.labels instead
      ## @param metrics.serviceMonitor.labels Used to pass Labels that are required by the installed Prometheus Operator
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
      ##
      labels: {}

      ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""